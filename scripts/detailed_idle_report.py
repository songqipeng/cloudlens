#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå½“å‰æ•°æ®åº“ç”Ÿæˆæ‰€æœ‰ç§Ÿæˆ·é—²ç½®èµ„æºè¯¦ç»†æŠ¥å‘Š
"""

import os
import sqlite3
import json
from datetime import datetime
from collections import defaultdict


def load_config():
    """åŠ è½½é…ç½®"""
    try:
        with open("config.json", "r") as f:
            return json.load(f)
    except:
        return {"tenants": {"ydzn": {"display_name": "ç¾Šå°å’©æ•°ç§‘"}, "zmyc": {"display_name": "ZMYC"}, "cf": {"display_name": "CFç§Ÿæˆ·"}}}


def analyze_ecs_metrics():
    """åˆ†æECSç›‘æ§æŒ‡æ ‡"""
    db_file = "ecs_monitoring_data_fixed.db"
    if not os.path.exists(db_file):
        return []
    
    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        
        # æŸ¥è¯¢æ¯ä¸ªå®ä¾‹çš„CPUå¹³å‡ä½¿ç”¨ç‡(å¦‚æœæœ‰CPUUtilizationæŒ‡æ ‡)
        query = """
        SELECT 
            i.instance_id,
            i.instance_name,
            i.instance_type,
            i.region,
            i.status,
            m.metric_name,
            AVG(m.metric_value) as avg_value,
            COUNT(*) as data_points
        FROM instances i
        LEFT JOIN monitoring_data m ON i.instance_id = m.instance_id
        WHERE m.timestamp >= datetime('now', '-7 days')
          AND m.metric_name IN ('CPUUtilization', 'cpu_idle', 'cpu.total')
        GROUP BY i.instance_id, m.metric_name
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        # æŒ‰å®ä¾‹IDåˆ†ç»„ç»Ÿè®¡
        instance_metrics = defaultdict(lambda: {"metrics": {}, "info": {}})
        
        for row in results:
            instance_id = row[0]
            instance_metrics[instance_id]["info"] = {
                "instance_id": row[0],
                "instance_name": row[1] or "æœªå‘½å",
                "instance_type": row[2],
                "region": row[3],
                "status": row[4]
            }
            instance_metrics[instance_id]["metrics"][row[5]] = {
                "avg_value": round(row[6], 2),
                "data_points": row[7]
            }
        
        # ä¹ŸæŸ¥è¯¢æ²¡æœ‰ç›‘æ§æ•°æ®çš„å®ä¾‹
        cursor.execute("""
            SELECT instance_id, instance_name, instance_type, region, status
            FROM instances
            WHERE instance_id NOT IN (
                SELECT DISTINCT instance_id 
                FROM monitoring_data 
                WHERE timestamp >= datetime('now', '-7 days')
            )
        """)
        
        no_metrics = cursor.fetchall()
        
        conn.close()
        
        return instance_metrics, no_metrics
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢ECSæ•°æ®å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {}, []


def analyze_rds_metrics():
    """åˆ†æRDSç›‘æ§æŒ‡æ ‡"""
    db_file = "rds_monitoring_data.db"
    if not os.path.exists(db_file):
        return []
    
    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        
        # æŸ¥è¯¢RDSå®ä¾‹åŠå…¶ç›‘æ§æ•°æ®
        query = """
        SELECT 
            i.instance_id,
            i.instance_name,
            i.engine,
            i.engine_version,
            i.region,
            i.status,
            m.metric_name,
            AVG(m.metric_value) as avg_value,
            COUNT(*) as data_points
        FROM rds_instances i
        LEFT JOIN rds_monitoring_data m ON i.instance_id = m.instance_id
        WHERE m.timestamp >= unixepoch('now', '-7 days')
          AND m.metric_name IN ('CPUUtilization', 'ConnectionUsage', 'ActiveConnections')
        GROUP BY i.instance_id, m.metric_name
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        instance_metrics = defaultdict(lambda: {"metrics": {}, "info": {}})
        
        for row in results:
            instance_id = row[0]
            instance_metrics[instance_id]["info"] = {
                "instance_id": row[0],
                "instance_name": row[1] or "æœªå‘½å",
                "engine": f"{row[2]} {row[3]}",
                "region": row[4],
                "status": row[5]
            }
            instance_metrics[instance_id]["metrics"][row[6]] = {
                "avg_value": round(row[7], 2),
                "data_points": row[8]
            }
        
        conn.close()
        
        return instance_metrics
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢RDSæ•°æ®å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {}


def generate_detailed_report():
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    print("="*100)
    print("ğŸ“Š æ‰€æœ‰ç§Ÿæˆ·é—²ç½®èµ„æºè¯¦ç»†åˆ†ææŠ¥å‘Š")
    print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*100)
    
    config = load_config()
    tenants = config.get("tenants", {})
    
    print(f"\nğŸ¢ å·²é…ç½®ç§Ÿæˆ· ({len(tenants)} ä¸ª):")
    for tenant_name, tenant_config in tenants.items():
        display_name = tenant_config.get("display_name", tenant_name)
        print(f"  â€¢ {display_name} ({tenant_name})")
    
    # èµ„æºæ€»é‡ç»Ÿè®¡
    print("\n" + "="*100)
    print("ğŸ“¦ èµ„æºæ€»é‡ç»Ÿè®¡")
    print("="*100)
    
    db_stats = {
        "ECSå®ä¾‹": ("ecs_monitoring_data_fixed.db", "SELECT COUNT(*) FROM instances"),
        "RDSå®ä¾‹": ("rds_monitoring_data.db", "SELECT COUNT(*) FROM rds_instances"),
        "SLBå®ä¾‹": ("slb_monitoring_data.db", "SELECT COUNT(*) FROM slb_instances"),
        "DNSåŸŸå": ("dns_monitoring_data.db", "SELECT COUNT(*) FROM dns_domains"),
        "DNSè®°å½•": ("dns_monitoring_data.db", "SELECT COUNT(*) FROM dns_records"),
    }
    
    total_resources = {}
    for resource_type, (db_file, query) in db_stats.items():
        if os.path.exists(db_file):
            try:
                conn = sqlite3.connect(db_file)
                cursor = conn.cursor()
                cursor.execute(query)
                count = cursor.fetchone()[0]
                total_resources[resource_type] = count
                conn.close()
            except:
                total_resources[resource_type] = 0
        else:
            total_resources[resource_type] = 0
    
    for resource_type, count in total_resources.items():
        print(f"  {resource_type:<15s}: {count:>6d} ä¸ª")
    
    # ECSè¯¦ç»†åˆ†æ
    print("\n" + "="*100)
    print("ğŸ–¥ï¸  ECSå®ä¾‹è¯¦ç»†åˆ†æ (è¿‡å»7å¤©ç›‘æ§æ•°æ®)")
    print("="*100)
    
    ecs_metrics, ecs_no_metrics = analyze_ecs_metrics()
    
    if ecs_metrics:
        print(f"\næœ‰ç›‘æ§æ•°æ®çš„å®ä¾‹: {len(ecs_metrics)} ä¸ª")
        print("-"*100)
        
        # ç»Ÿè®¡ä½ä½¿ç”¨ç‡å®ä¾‹
        low_cpu_instances = []
        for instance_id, data in ecs_metrics.items():
            info = data["info"]
            metrics = data["metrics"]
            
            # æŸ¥æ‰¾CPUç›¸å…³æŒ‡æ ‡
            cpu_value = None
            for metric_name in ['CPUUtilization', 'cpu_idle', 'cpu.total']:
                if metric_name in metrics:
                    cpu_value = metrics[metric_name]["avg_value"]
                    break
            
            if cpu_value is not None and cpu_value < 10:
                low_cpu_instances.append({
                    **info,
                    "cpu_avg": cpu_value,
                    "metrics": metrics
                })
        
        if low_cpu_instances:
            print(f"\nğŸ”´ ä½CPUä½¿ç”¨ç‡å®ä¾‹ (CPU < 10%): {len(low_cpu_instances)} ä¸ª")
            print(f"{'å®ä¾‹åç§°':<30s} {'å®ä¾‹ID':<20s} {'ç±»å‹':<15s} {'åœ°åŸŸ':<15s} {'å¹³å‡CPU%':<10s}")
            print("-"*100)
            for inst in low_cpu_instances[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                print(f"{inst['instance_name'][:29]:<30s} "
                      f"{inst['instance_id'][:19]:<20s} "
                      f"{inst['instance_type'][:14]:<15s} "
                      f"{inst['region'][:14]:<15s} "
                      f"{inst['cpu_avg']:>9.2f}%")
            
            if len(low_cpu_instances) > 20:
                print(f"... è¿˜æœ‰ {len(low_cpu_instances) - 20} ä¸ªå®ä¾‹æœªæ˜¾ç¤º")
        else:
            print("\nâœ… æœªå‘ç°æ˜æ˜¾ä½CPUä½¿ç”¨ç‡å®ä¾‹")
    
    if ecs_no_metrics:
        print(f"\nâš ï¸  æ— ç›‘æ§æ•°æ®çš„å®ä¾‹: {len(ecs_no_metrics)} ä¸ª")
        print("-"*100)
        print(f"{'å®ä¾‹åç§°':<30s} {'å®ä¾‹ID':<20s} {'ç±»å‹':<15s} {'åœ°åŸŸ':<15s} {'çŠ¶æ€':<10s}")
        print("-"*100)
        for row in ecs_no_metrics[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            print(f"{(row[1] or 'æœªå‘½å')[:29]:<30s} "
                  f"{row[0][:19]:<20s} "
                  f"{row[2][:14]:<15s} "
                  f"{row[3][:14]:<15s} "
                  f"{row[4]:<10s}")
        
        if len(ecs_no_metrics) > 20:
            print(f"... è¿˜æœ‰ {len(ecs_no_metrics) - 20} ä¸ªå®ä¾‹æœªæ˜¾ç¤º")
    
    # RDSè¯¦ç»†åˆ†æ
    print("\n" + "="*100)
    print("ğŸ—„ï¸  RDSå®ä¾‹è¯¦ç»†åˆ†æ (è¿‡å»7å¤©ç›‘æ§æ•°æ®)")
    print("="*100)
    
    rds_metrics = analyze_rds_metrics()
    
    if rds_metrics:
        print(f"\næœ‰ç›‘æ§æ•°æ®çš„å®ä¾‹: {len(rds_metrics)} ä¸ª")
        print("-"*100)
        
        # ç»Ÿè®¡ä½ä½¿ç”¨ç‡å®ä¾‹
        low_util_instances = []
        for instance_id, data in rds_metrics.items():
            info = data["info"]
            metrics = data["metrics"]
            
            cpu_value = metrics.get("CPUUtilization", {}).get("avg_value")
            conn_value = metrics.get("ActiveConnections", {}).get("avg_value", metrics.get("ConnectionUsage", {}).get("avg_value"))
            
            if (cpu_value is not None and cpu_value < 10) or (conn_value is not None and conn_value < 5):
                low_util_instances.append({
                    **info,
                    "cpu_avg": cpu_value,
                    "conn_avg": conn_value,
                    "metrics": metrics
                })
        
        if low_util_instances:
            print(f"\nğŸ”´ ä½ä½¿ç”¨ç‡RDSå®ä¾‹: {len(low_util_instances)} ä¸ª")
            print(f"{'å®ä¾‹åç§°':<30s} {'å¼•æ“':<20s} {'åœ°åŸŸ':<15s} {'CPU%':<10s} {'è¿æ¥æ•°':<10s}")
            print("-"*100)
            for inst in low_util_instances[:20]:
                cpu_str = f"{inst['cpu_avg']:.2f}" if inst['cpu_avg'] is not None else "N/A"
                conn_str = f"{inst['conn_avg']:.2f}" if inst['conn_avg'] is not None else "N/A"
                print(f"{inst['instance_name'][:29]:<30s} "
                      f"{inst['engine'][:19]:<20s} "
                      f"{inst['region'][:14]:<15s} "
                      f"{cpu_str:<10s} "
                      f"{conn_str:<10s}")
            
            if len(low_util_instances) > 20:
                print(f"... è¿˜æœ‰ {len(low_util_instances) - 20} ä¸ªå®ä¾‹æœªæ˜¾ç¤º")
        else:
            print("\nâœ… æœªå‘ç°æ˜æ˜¾ä½ä½¿ç”¨ç‡å®ä¾‹")
    else:
        print("\nğŸ’¡ æ— ç›‘æ§æ•°æ®,å»ºè®®è¿è¡Œå®Œæ•´åˆ†æ")
    
    # å»ºè®®
    print("\n" + "="*100)
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print("="*100)
    print("""
1. ğŸ” å®šæœŸç›‘æ§: å»ºç«‹å®šæœŸåˆ†ææœºåˆ¶,æ¯å‘¨æŸ¥çœ‹èµ„æºä½¿ç”¨æƒ…å†µ
   
2. ğŸ’° æˆæœ¬ä¼˜åŒ–:
   - ä½CPUä½¿ç”¨ç‡ECS: è€ƒè™‘é™é…æˆ–é‡Šæ”¾
   - ä½è¿æ¥æ•°RDS: è¯„ä¼°æ˜¯å¦å¯ä»¥åˆå¹¶æˆ–ç¼©å®¹
   - æ— ç›‘æ§æ•°æ®å®ä¾‹: å¯èƒ½æ˜¯åœæœºæˆ–é…ç½®é—®é¢˜
   
3. ğŸ› ï¸ è¿è¡Œå®Œæ•´åˆ†æ:
   - æ‰€æœ‰ç§Ÿæˆ·: python3 analyze_all_tenants.py
   - å•ä¸ªç§Ÿæˆ·: python3 main.py [ç§Ÿæˆ·å] cru all
   - ç‰¹å®šèµ„æº: python3 main.py [ç§Ÿæˆ·å] cru [èµ„æºç±»å‹]
   
4. ğŸ“Š æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š:
   - ç£ç›˜åˆ†æ: python3 main.py [ç§Ÿæˆ·å] cru disk
   - ç½‘ç»œèµ„æº: python3 main.py [ç§Ÿæˆ·å] network
   - è´¹ç”¨åˆ†æ: python3 main.py [ç§Ÿæˆ·å] cost
""")
    
    print("="*100)
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("="*100)


if __name__ == "__main__":
    generate_detailed_report()
