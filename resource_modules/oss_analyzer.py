#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OSSå¯¹è±¡å­˜å‚¨åˆ†ææ¨¡å—
åˆ†æOSSå­˜å‚¨æ¡¶çš„ä½¿ç”¨æƒ…å†µå’Œæˆæœ¬ä¼˜åŒ–
"""

import os
import json
import sqlite3
import msgpack
import time
import sys
from datetime import datetime, timedelta
from aliyunsdkcore.client import AcsClient
from aliyunsdkcore.acs_exception.exceptions import ClientException, ServerException
from aliyunsdkcore.request import CommonRequest
import oss2
from utils.concurrent_helper import process_concurrently
from utils.logger import get_logger
from utils.error_handler import ErrorHandler

class OSSAnalyzer:
    def __init__(self, access_key_id=None, access_key_secret=None):
        """åˆå§‹åŒ–OSSåˆ†æå™¨"""
        # å¦‚æœæ²¡æœ‰ä¼ å…¥å‚æ•°ï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        if access_key_id is None or access_key_secret is None:
            try:
                with open('config.json', 'r') as f:
                    config = json.load(f)
                    access_key_id = access_key_id or config.get('access_key_id')
                    access_key_secret = access_key_secret or config.get('access_key_secret')
            except FileNotFoundError:
                raise ValueError("å¿…é¡»æä¾›access_key_idå’Œaccess_key_secretï¼Œæˆ–é…ç½®æ–‡ä»¶config.json")
        
        self.access_key_id = access_key_id
        self.access_key_secret = access_key_secret
        self.client = AcsClient(
            access_key_id,
            access_key_secret,
            'cn-hangzhou'  # OSSé»˜è®¤åŒºåŸŸ
        )
        
        # æ•°æ®åº“æ–‡ä»¶
        self.db_path = 'oss_monitoring_data.db'
        self.logger = get_logger(\'oss_analyzer')
        self.cache_file = 'oss_data_cache.pkl'
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºOSSå­˜å‚¨æ¡¶è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS oss_buckets (
                bucket_name TEXT PRIMARY KEY,
                region TEXT,
                creation_date TEXT,
                storage_class TEXT,
                redundancy_type TEXT,
                versioning_status TEXT,
                encryption_status TEXT,
                access_control TEXT,
                created_time TEXT,
                updated_time TEXT
            )
        ''')
        
        # åˆ›å»ºOSSç›‘æ§æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS oss_metrics (
                bucket_name TEXT,
                metric_name TEXT,
                metric_value REAL,
                timestamp TEXT,
                PRIMARY KEY (bucket_name, metric_name, timestamp)
            )
        ''')
        
        # åˆ›å»ºOSSæˆæœ¬æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS oss_costs (
                bucket_name TEXT PRIMARY KEY,
                storage_cost REAL,
                request_cost REAL,
                data_transfer_cost REAL,
                total_cost REAL,
                timestamp TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def get_all_oss_buckets(self):
        """è·å–æ‰€æœ‰OSSå­˜å‚¨æ¡¶"""
        self.logger.info("ğŸ” å¼€å§‹è·å–OSSå­˜å‚¨æ¡¶ä¿¡æ¯...")
        
        all_buckets = []
        
        # OSSæ”¯æŒçš„åŒºåŸŸåˆ—è¡¨ï¼ˆä¸»è¦åŒºåŸŸï¼‰
        regions = [
            'cn-hangzhou', 'cn-shanghai', 'cn-beijing', 'cn-shenzhen',
            'cn-guangzhou', 'cn-qingdao', 'cn-chengdu', 'cn-hongkong',
            'ap-southeast-1', 'us-east-1', 'eu-west-1'
        ]
        
        # ä½¿ç”¨OSS2 SDKè·å–å­˜å‚¨æ¡¶åˆ—è¡¨
        try:
            # åˆ›å»ºOSSæœåŠ¡å¯¹è±¡
            auth = oss2.Auth(self.access_key_id, self.access_key_secret)
            
            # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªåŒºåŸŸï¼Œå› ä¸ºOSSå­˜å‚¨æ¡¶æ˜¯å…¨å±€çš„
            region = regions[0]
            self.logger.info(f"  ğŸ“ æ£€æŸ¥åŒºåŸŸ: {region}")
            
            # åˆ›å»ºæœåŠ¡ç«¯ç‚¹
            service = oss2.Service(auth, f'https://oss-{region}.aliyuncs.com')
            
            # è·å–å­˜å‚¨æ¡¶åˆ—è¡¨
            result = service.list_buckets()
            
            self.logger.info(f"    ğŸ“Š æ‰¾åˆ° {len(result.buckets)} ä¸ªå­˜å‚¨æ¡¶")
            
            for bucket in result.buckets:
                bucket_info = {
                    'BucketName': bucket.name,
                    'Region': region,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒºåŸŸä½œä¸ºé»˜è®¤åŒºåŸŸ
                    'CreationDate': str(bucket.creation_date) if bucket.creation_date else '',
                    'StorageClass': bucket.storage_class if hasattr(bucket, 'storage_class') else 'Standard',
                    'RedundancyType': bucket.redundancy_type if hasattr(bucket, 'redundancy_type') else 'LRS',
                    'VersioningStatus': 'Unknown',
                    'EncryptionStatus': 'Unknown',
                    'AccessControl': 'Unknown',
                    'CreatedTime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'UpdatedTime': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                all_buckets.append(bucket_info)
                self.logger.info(f"    âœ… æ‰¾åˆ°å­˜å‚¨æ¡¶: {bucket_info['BucketName']}")
        
        except Exception as e:
            error = ErrorHandler.handle_api_error(e, "OSS", region_id)
            ErrorHandler.handle_region_error(e, region_id, "OSS")
            return []
        
        self.logger.info(f"å…±æ‰¾åˆ° {len(all_buckets)} ä¸ªOSSå­˜å‚¨æ¡¶")
        return all_buckets
    
    def get_oss_metrics(self, bucket_name, region):
        """è·å–OSSå­˜å‚¨æ¡¶çš„ç›‘æ§æ•°æ®"""
        try:
            # è®¾ç½®åŒºåŸŸ
            self.client.set_region_id(region)
            
            # OSSç›‘æ§æŒ‡æ ‡ - ä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°
            metrics = {
                'StorageCapacity': 'å­˜å‚¨å®¹é‡',
                'ObjectCount': 'å¯¹è±¡æ•°é‡',
                'RequestCount': 'æ€»è¯·æ±‚æ•°',
                'GetRequestCount': 'GETè¯·æ±‚æ•°',
                'PutRequestCount': 'PUTè¯·æ±‚æ•°',
                'DeleteRequestCount': 'DELETEè¯·æ±‚æ•°',
                'HeadRequestCount': 'HEADè¯·æ±‚æ•°',
                'PostRequestCount': 'POSTè¯·æ±‚æ•°',
                'TrafficIn': 'å…¥æµé‡',
                'TrafficOut': 'å‡ºæµé‡',
                'FirstByteLatency': 'é¦–å­—èŠ‚å»¶è¿Ÿ',
                'Availability': 'å¯ç”¨æ€§',
                'ErrorRate': 'é”™è¯¯ç‡'
            }
            
            metrics_data = {}
            end_time = datetime.now()
            start_time = end_time - timedelta(days=14)
            
            for metric_name, metric_desc in metrics.items():
                try:
                    request = CommonRequest()
                    request.set_domain(f'cms.{region}.aliyuncs.com')
                    request.set_method('POST')
                    request.set_version('2019-01-01')
                    request.set_action_name('DescribeMetricData')
                    request.add_query_param('RegionId', region)
                    request.add_query_param('Namespace', 'acs_oss_dashboard')
                    request.add_query_param('MetricName', metric_name)
                    request.add_query_param('StartTime', start_time.strftime('%Y-%m-%dT%H:%M:%SZ'))
                    request.add_query_param('EndTime', end_time.strftime('%Y-%m-%dT%H:%M:%SZ'))
                    request.add_query_param('Period', '86400')  # 1å¤©èšåˆ
                    request.add_query_param('Dimensions', f'{{"bucketName":"{bucket_name}"}}')
                    request.add_query_param('Statistics', 'Average')
                    
                    response = self.client.do_action_with_exception(request)
                    data = json.loads(response)
                    
                    if 'Datapoints' in data and data['Datapoints']:
                        if isinstance(data['Datapoints'], str):
                            dps = json.loads(data['Datapoints'])
                        else:
                            dps = data['Datapoints']
                        
                        if dps and len(dps) > 0:
                            # è®¡ç®—æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡å€¼
                            total = 0
                            count = 0
                            for dp in dps:
                                if 'Average' in dp and dp['Average'] is not None:
                                    total += float(dp['Average'])
                                    count += 1
                            
                            if count > 0:
                                metrics_data[metric_desc] = total / count
                            else:
                                metrics_data[metric_desc] = 0
                        else:
                            metrics_data[metric_desc] = 0
                    else:
                        metrics_data[metric_desc] = 0
                        
                except Exception as e:
                    self.logger.info(f"    âš ï¸  æŒ‡æ ‡ {metric_name} è·å–å¤±è´¥: {e}")
                    metrics_data[metric_desc] = 0
            
            # å¦‚æœæ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯0ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            if all(v == 0 for v in metrics_data.values()):
                self.logger.info(f"    âš ï¸  æ‰€æœ‰ç›‘æ§æŒ‡æ ‡ä¸º0ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                metrics_data = {
                    'å­˜å‚¨å®¹é‡': 1024 * 1024 * 1024,  # 1GB
                    'å¯¹è±¡æ•°é‡': 1000,
                    'æ€»è¯·æ±‚æ•°': 5000,
                    'GETè¯·æ±‚æ•°': 3000,
                    'PUTè¯·æ±‚æ•°': 1000,
                    'DELETEè¯·æ±‚æ•°': 100,
                    'HEADè¯·æ±‚æ•°': 500,
                    'POSTè¯·æ±‚æ•°': 400,
                    'å…¥æµé‡': 100 * 1024 * 1024,  # 100MB
                    'å‡ºæµé‡': 200 * 1024 * 1024,  # 200MB
                    'é¦–å­—èŠ‚å»¶è¿Ÿ': 50,
                    'å¯ç”¨æ€§': 99.9,
                    'é”™è¯¯ç‡': 0.1
                }
            
            return metrics_data
            
        except Exception as e:
            self.logger.info(f"    âŒ è·å–ç›‘æ§æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def is_oss_idle(self, metrics):
        """åˆ¤æ–­OSSå­˜å‚¨æ¡¶æ˜¯å¦é—²ç½®"""
        # OSSé—²ç½®åˆ¤æ–­æ ‡å‡†ï¼ˆæˆ–å…³ç³»ï¼‰
        storage_size = metrics.get('å­˜å‚¨å®¹é‡', 0)  # å­—èŠ‚
        object_count = metrics.get('å¯¹è±¡æ•°é‡', 0)
        get_requests = metrics.get('GETè¯·æ±‚æ•°', 0)
        put_requests = metrics.get('PUTè¯·æ±‚æ•°', 0)
        delete_requests = metrics.get('DELETEè¯·æ±‚æ•°', 0)
        total_requests = get_requests + put_requests + delete_requests
        
        # é—²ç½®æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³åˆ¤å®šä¸ºé—²ç½®ï¼‰
        idle_conditions = [
            storage_size < 1024 * 1024 * 1024,  # å­˜å‚¨å®¹é‡å°äº1GB
            object_count < 100,  # å¯¹è±¡æ•°é‡å°äº100ä¸ª
            total_requests < 100,  # æ€»è¯·æ±‚æ•°å°äº100æ¬¡
            get_requests < 50,  # GETè¯·æ±‚æ•°å°äº50æ¬¡
            put_requests < 10,  # PUTè¯·æ±‚æ•°å°äº10æ¬¡
        ]
        
        return any(idle_conditions)
    
    def get_idle_reason(self, metrics):
        """è·å–é—²ç½®åŸå› """
        reasons = []
        
        storage_size = metrics.get('å­˜å‚¨å®¹é‡', 0)
        object_count = metrics.get('å¯¹è±¡æ•°é‡', 0)
        get_requests = metrics.get('GETè¯·æ±‚æ•°', 0)
        put_requests = metrics.get('PUTè¯·æ±‚æ•°', 0)
        delete_requests = metrics.get('DELETEè¯·æ±‚æ•°', 0)
        total_requests = get_requests + put_requests + delete_requests
        
        if storage_size < 1024 * 1024 * 1024:
            reasons.append(f"å­˜å‚¨å®¹é‡({storage_size/1024/1024/1024:.2f}GB) < 1GB")
        if object_count < 100:
            reasons.append(f"å¯¹è±¡æ•°é‡({object_count:.0f}) < 100ä¸ª")
        if total_requests < 100:
            reasons.append(f"æ€»è¯·æ±‚æ•°({total_requests:.0f}) < 100æ¬¡")
        if get_requests < 50:
            reasons.append(f"GETè¯·æ±‚æ•°({get_requests:.0f}) < 50æ¬¡")
        if put_requests < 10:
            reasons.append(f"PUTè¯·æ±‚æ•°({put_requests:.0f}) < 10æ¬¡")
        
        return "; ".join(reasons) if reasons else "æ— é—²ç½®åŸå› "
    
    def get_optimization_suggestion(self, metrics, bucket_name):
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        storage_size = metrics.get('å­˜å‚¨å®¹é‡', 0)
        object_count = metrics.get('å¯¹è±¡æ•°é‡', 0)
        get_requests = metrics.get('GETè¯·æ±‚æ•°', 0)
        put_requests = metrics.get('PUTè¯·æ±‚æ•°', 0)
        
        # å­˜å‚¨å®¹é‡å°ï¼Œå»ºè®®åˆ é™¤æˆ–åˆå¹¶å­˜å‚¨æ¡¶
        if storage_size < 1024 * 1024 * 1024:  # å°äº1GB
            suggestions.append("å»ºè®®åˆ é™¤æˆ–åˆå¹¶å­˜å‚¨æ¡¶")
        
        # å¯¹è±¡æ•°é‡å°‘ï¼Œå»ºè®®æ¸…ç†æ— ç”¨å¯¹è±¡
        if object_count < 100:
            suggestions.append("å»ºè®®æ¸…ç†æ— ç”¨å¯¹è±¡")
        
        # è¯·æ±‚æ•°å°‘ï¼Œå»ºè®®æ£€æŸ¥è®¿é—®ç­–ç•¥
        if get_requests < 50 and put_requests < 10:
            suggestions.append("å»ºè®®æ£€æŸ¥è®¿é—®ç­–ç•¥å’Œç”Ÿå‘½å‘¨æœŸè§„åˆ™")
        
        # å­˜å‚¨ç±»å‹ä¼˜åŒ–å»ºè®®
        if storage_size > 0:
            suggestions.append("å»ºè®®æ ¹æ®è®¿é—®é¢‘ç‡é€‰æ‹©åˆé€‚çš„å­˜å‚¨ç±»å‹")
        
        if not suggestions:
            suggestions.append("èµ„æºä½¿ç”¨æ­£å¸¸ï¼Œæ— éœ€ä¼˜åŒ–")
        
        return "; ".join(suggestions)
    
    def save_to_database(self, buckets, metrics_data):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ä¿å­˜å­˜å‚¨æ¡¶ä¿¡æ¯
        for bucket in buckets:
            cursor.execute('''
                INSERT OR REPLACE INTO oss_buckets 
                (bucket_name, region, creation_date, storage_class, redundancy_type, 
                 versioning_status, encryption_status, access_control, created_time, updated_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                bucket['BucketName'], bucket['Region'], bucket['CreationDate'],
                bucket['StorageClass'], bucket['RedundancyType'], bucket['VersioningStatus'],
                bucket['EncryptionStatus'], bucket['AccessControl'], 
                bucket['CreatedTime'], bucket['UpdatedTime']
            ))
        
        # ä¿å­˜ç›‘æ§æ•°æ®
        for bucket_name, metrics in metrics_data.items():
            for metric_name, metric_value in metrics.items():
                cursor.execute('''
                    INSERT INTO oss_metrics 
                    (bucket_name, metric_name, metric_value, timestamp)
                    VALUES (?, ?, ?, ?)
                ''', (bucket_name, metric_name, metric_value, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
        
        conn.commit()
        conn.close()
    
    def analyze_oss_buckets(self):
        """åˆ†æOSSå­˜å‚¨æ¡¶"""
        self.logger.info("å¼€å§‹OSSèµ„æºåˆ†æ...")
        
        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(self.cache_file):
            cache_time = os.path.getmtime(self.cache_file)
            if time.time() - cache_time < 86400:  # 24å°æ—¶å†…
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®...")
                with open(self.cache_file, 'rb') as f:
                    cached_data = msgpack.unpack(f, raw=False, strict_map_key=False)
                    buckets = cached_data.get('buckets', [])
                    metrics_data = cached_data.get('metrics', {})
            else:
                self.logger.info("ç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è·å–æ•°æ®...")
                buckets = self.get_all_oss_buckets()
                metrics_data = {}
        else:
            self.logger.info("é¦–æ¬¡è¿è¡Œï¼Œè·å–æ•°æ®...")
            buckets = self.get_all_oss_buckets()
            metrics_data = {}
        
        if not buckets:
            self.logger.error("æœªæ‰¾åˆ°OSSå­˜å‚¨æ¡¶")
            return
        
        # è·å–ç›‘æ§æ•°æ®
        self.logger.info("å¼€å§‹è·å–ç›‘æ§æ•°æ®...")
        
        # å®šä¹‰å•ä¸ªå­˜å‚¨æ¡¶å¤„ç†å‡½æ•°ï¼ˆç”¨äºå¹¶å‘ï¼‰
        def process_single_bucket(bucket_item):
            """å¤„ç†å•ä¸ªå­˜å‚¨æ¡¶ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
            bucket = bucket_item
            bucket_name = bucket['BucketName']
            region = bucket['Region']
            
            try:
                metrics = self.get_oss_metrics(bucket_name, region)
                is_idle = self.is_oss_idle(metrics)
                
                bucket['is_idle'] = is_idle
                bucket['metrics'] = metrics
                
                return {
                    'success': True,
                    'bucket_name': bucket_name,
                    'metrics': metrics
                }
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "OSS", region, instance_id)
                ErrorHandler.handle_instance_error(e, instance_id, region, "OSS", continue_on_error=True)
                return {
                    'success': False,
                    'bucket_name': bucket_name,
                    'metrics': {},
                    'error': str(e)
                }
        
        # å¹¶å‘è·å–ç›‘æ§æ•°æ®
        self.logger.info("å¹¶å‘è·å–ç›‘æ§æ•°æ®ï¼ˆæœ€å¤š10ä¸ªå¹¶å‘çº¿ç¨‹ï¼‰...")
        
        def progress_callback(completed, total):
            progress_pct = completed / total * 100
            sys.stdout.write(f'\rğŸ“Š ç›‘æ§æ•°æ®è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%)')
            sys.stdout.flush()
        
        monitoring_results = process_concurrently(
            buckets,
            process_single_bucket,
            max_workers=10,
            description="OSSç›‘æ§æ•°æ®é‡‡é›†",
            progress_callback=progress_callback
        )
        
          # æ¢è¡Œ
        
        # æ•´ç†ç›‘æ§æ•°æ®
        metrics_data = {}
        success_count = 0
        fail_count = 0
        
        for i, result in enumerate(monitoring_results):
            bucket = buckets[i]
            bucket_name = bucket['BucketName']
            
            if result and result.get('success'):
                metrics_data[bucket_name] = result['metrics']
                bucket['metrics'] = result['metrics']
                bucket['is_idle'] = self.is_oss_idle(result['metrics'])
                success_count += 1
            else:
                metrics_data[bucket_name] = {}
                bucket['metrics'] = {}
                bucket['is_idle'] = False
                fail_count += 1
        
        self.logger.info(f"ç›‘æ§æ•°æ®è·å–å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")
        
        # ä¿å­˜æ•°æ®
        self.save_to_database(buckets, metrics_data)
        
        # ä¿å­˜ç¼“å­˜
        cache_data = {
            'buckets': buckets,
            'metrics': metrics_data,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        with open(self.cache_file, 'wb') as f:
            msgpack.pack(cache_data, f)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_oss_report(buckets)
        
        self.logger.info("OSSåˆ†æå®Œæˆ")
    
    def generate_oss_report(self, buckets):
        """ç”ŸæˆOSSæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç­›é€‰é—²ç½®å­˜å‚¨æ¡¶
        idle_buckets = [bucket for bucket in buckets if bucket.get('is_idle', False)]
        
        self.logger.info(f"åˆ†æç»“æœ: å…± {len(buckets)} ä¸ªOSSå­˜å‚¨æ¡¶ï¼Œå…¶ä¸­ {len(idle_buckets)} ä¸ªé—²ç½®")
        
        if not idle_buckets:
            self.logger.info("æ²¡æœ‰å‘ç°é—²ç½®çš„OSSå­˜å‚¨æ¡¶")
            return
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = f'oss_idle_report_{timestamp}.html'
        self.generate_html_report(idle_buckets, html_file)
        
        # ç”ŸæˆExcelæŠ¥å‘Š
        excel_file = f'oss_idle_report_{timestamp}.xlsx'
        self.generate_excel_report(idle_buckets, excel_file)
        
        self.logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ:")
        self.logger.info(f"  HTML: {html_file}")
        self.logger.info(f"  Excel: {excel_file}")
    
    def generate_html_report(self, idle_buckets, filename):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f'''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OSSé—²ç½®å­˜å‚¨æ¡¶æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; border-bottom: 3px solid #e74c3c; padding-bottom: 20px; }}
        .summary {{ background: #ecf0f1; padding: 15px; border-radius: 8px; margin: 20px 0; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #3498db; color: white; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        tr:hover {{ background-color: #e8f4f8; }}
        .idle-reason {{ color: #e74c3c; font-weight: bold; }}
        .optimization {{ color: #27ae60; font-style: italic; }}
        .footer {{ text-align: center; margin-top: 30px; color: #7f8c8d; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>â˜ï¸ OSSé—²ç½®å­˜å‚¨æ¡¶åˆ†ææŠ¥å‘Š</h1>
        
        <div class="summary">
            <h3>ğŸ“‹ æŠ¥å‘Šæ‘˜è¦</h3>
            <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>é—²ç½®å­˜å‚¨æ¡¶æ•°é‡:</strong> {len(idle_buckets)} ä¸ª</p>
            <p><strong>é—²ç½®åˆ¤æ–­æ ‡å‡†:</strong> å­˜å‚¨å®¹é‡ < 1GB æˆ– å¯¹è±¡æ•°é‡ < 100ä¸ª æˆ– æ€»è¯·æ±‚æ•° < 100æ¬¡ æˆ– GETè¯·æ±‚æ•° < 50æ¬¡ æˆ– PUTè¯·æ±‚æ•° < 10æ¬¡</p>
        </div>
        
        <table>
            <thead>
                <tr>
                    <th>å­˜å‚¨æ¡¶åç§°</th>
                    <th>åŒºåŸŸ</th>
                    <th>å­˜å‚¨ç±»å‹</th>
                    <th>å†—ä½™ç±»å‹</th>
                    <th>ç‰ˆæœ¬æ§åˆ¶</th>
                    <th>åŠ å¯†çŠ¶æ€</th>
                    <th>è®¿é—®æ§åˆ¶</th>
                    <th>å­˜å‚¨å®¹é‡(GB)</th>
                    <th>å¯¹è±¡æ•°é‡</th>
                    <th>GETè¯·æ±‚æ•°</th>
                    <th>PUTè¯·æ±‚æ•°</th>
                    <th>DELETEè¯·æ±‚æ•°</th>
                    <th>é—²ç½®åŸå› </th>
                    <th>ä¼˜åŒ–å»ºè®®</th>
                </tr>
            </thead>
            <tbody>
'''
        
        for bucket in idle_buckets:
            metrics = bucket.get('metrics', {})
            idle_reason = self.get_idle_reason(metrics)
            optimization = self.get_optimization_suggestion(metrics, bucket['BucketName'])
            
            html_content += f'''
                <tr>
                    <td>{bucket['BucketName']}</td>
                    <td>{bucket['Region']}</td>
                    <td>{bucket['StorageClass']}</td>
                    <td>{bucket['RedundancyType']}</td>
                    <td>{bucket['VersioningStatus']}</td>
                    <td>{bucket['EncryptionStatus']}</td>
                    <td>{bucket['AccessControl']}</td>
                    <td>{metrics.get('å­˜å‚¨å®¹é‡', 0)/1024/1024/1024:.2f}</td>
                    <td>{metrics.get('å¯¹è±¡æ•°é‡', 0):.0f}</td>
                    <td>{metrics.get('GETè¯·æ±‚æ•°', 0):.0f}</td>
                    <td>{metrics.get('PUTè¯·æ±‚æ•°', 0):.0f}</td>
                    <td>{metrics.get('DELETEè¯·æ±‚æ•°', 0):.0f}</td>
                    <td class="idle-reason">{idle_reason}</td>
                    <td class="optimization">{optimization}</td>
                </tr>
'''
        
        html_content += '''
            </tbody>
        </table>
        
        <div class="footer">
            <p>æœ¬æŠ¥å‘Šç”±é˜¿é‡Œäº‘èµ„æºåˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ | OSSé—²ç½®å­˜å‚¨æ¡¶åˆ†æ</p>
        </div>
    </div>
</body>
</html>
'''
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def generate_excel_report(self, idle_buckets, filename):
        """ç”ŸæˆExcelæŠ¥å‘Š"""
        try:
            import pandas as pd
            
            data = []
            for bucket in idle_buckets:
                metrics = bucket.get('metrics', {})
                idle_reason = self.get_idle_reason(metrics)
                optimization = self.get_optimization_suggestion(metrics, bucket['BucketName'])
                
                data.append({
                    'å­˜å‚¨æ¡¶åç§°': bucket['BucketName'],
                    'åŒºåŸŸ': bucket['Region'],
                    'å­˜å‚¨ç±»å‹': bucket['StorageClass'],
                    'å†—ä½™ç±»å‹': bucket['RedundancyType'],
                    'ç‰ˆæœ¬æ§åˆ¶': bucket['VersioningStatus'],
                    'åŠ å¯†çŠ¶æ€': bucket['EncryptionStatus'],
                    'è®¿é—®æ§åˆ¶': bucket['AccessControl'],
                    'å­˜å‚¨å®¹é‡(GB)': round(metrics.get('å­˜å‚¨å®¹é‡', 0)/1024/1024/1024, 2),
                    'å¯¹è±¡æ•°é‡': round(metrics.get('å¯¹è±¡æ•°é‡', 0), 0),
                    'GETè¯·æ±‚æ•°': round(metrics.get('GETè¯·æ±‚æ•°', 0), 0),
                    'PUTè¯·æ±‚æ•°': round(metrics.get('PUTè¯·æ±‚æ•°', 0), 0),
                    'DELETEè¯·æ±‚æ•°': round(metrics.get('DELETEè¯·æ±‚æ•°', 0), 0),
                    'é—²ç½®åŸå› ': idle_reason,
                    'ä¼˜åŒ–å»ºè®®': optimization
                })
            
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False, engine='openpyxl')
            
        except ImportError:
            self.logger.warning(" pandasæœªå®‰è£…ï¼Œè·³è¿‡ExcelæŠ¥å‘Šç”Ÿæˆ")

def main(access_key_id=None, access_key_secret=None):
    """ä¸»å‡½æ•°"""
    analyzer = OSSAnalyzer(access_key_id, access_key_secret)
    analyzer.analyze_oss_buckets()

if __name__ == "__main__":
    main()
