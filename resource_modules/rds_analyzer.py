#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RDSèµ„æºåˆ†ææ¨¡å—
åˆ†æRDSå®ä¾‹çš„é—²ç½®æƒ…å†µï¼Œæä¾›ä¼˜åŒ–å»ºè®®
"""

import json
import sqlite3
import sys
import time
from datetime import datetime

import pandas as pd
from aliyunsdkcms.request.v20190101 import DescribeMetricDataRequest
from aliyunsdkcore.client import AcsClient
from aliyunsdkcore.request import CommonRequest
from aliyunsdkrds.request.v20140815 import DescribeDBInstancesRequest

from core.analyzer_registry import AnalyzerRegistry
from core.base_analyzer import BaseResourceAnalyzer
from core.db_manager import DatabaseManager
from core.report_generator import ReportGenerator
from utils.concurrent_helper import process_concurrently
from utils.error_handler import ErrorHandler
from utils.logger import get_logger


@AnalyzerRegistry.register("rds", "RDSæ•°æ®åº“", "ğŸ—„ï¸")
class RDSAnalyzer(BaseResourceAnalyzer):
    """RDSèµ„æºåˆ†æå™¨"""

    def __init__(self, access_key_id, access_key_secret, tenant_name=None):
        super().__init__(
            access_key_id=access_key_id,
            access_key_secret=access_key_secret,
            tenant_name=tenant_name or "default",
        )
        self.db_name = "rds_monitoring_data.db"
        self.logger = get_logger("rds_analyzer")
        # ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager(self.db_name)

    def get_resource_type(self) -> str:
        return "rds"

    def init_database(self):
        """åˆå§‹åŒ–RDSæ•°æ®åº“ï¼ˆä½¿ç”¨ç»Ÿä¸€DatabaseManagerï¼‰"""
        # å®šä¹‰é¢å¤–åˆ—
        extra_columns = {"engine": "TEXT", "engine_version": "TEXT", "instance_class": "TEXT"}

        self.db_manager.create_resource_table("rds", extra_columns)
        self.db_manager.create_monitoring_table("rds")
        self.logger.info("RDSæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    def get_all_regions(self):
        """è·å–æ‰€æœ‰å¯ç”¨åŒºåŸŸ"""
        client = AcsClient(self.access_key_id, self.access_key_secret, "cn-hangzhou")
        request = CommonRequest()
        request.set_domain("ecs.cn-hangzhou.aliyuncs.com")
        request.set_method("POST")
        request.set_version("2014-05-26")
        request.set_action_name("DescribeRegions")

        response = client.do_action_with_exception(request)
        data = json.loads(response)

        regions = []
        for region in data["Regions"]["Region"]:
            regions.append(region["RegionId"])

        return regions

    def get_instances(self, region: str):
        """è·å–æŒ‡å®šåŒºåŸŸçš„RDSå®ä¾‹ (BaseResourceAnalyzeræ¥å£)"""
        return self.get_rds_instances(region)

    def get_rds_instances(self, region_id):
        """è·å–æŒ‡å®šåŒºåŸŸçš„RDSå®ä¾‹"""
        try:
            client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
            request = DescribeDBInstancesRequest.DescribeDBInstancesRequest()
            request.set_PageSize(100)

            response = client.do_action_with_exception(request)
            data = json.loads(response)

            instances = []
            if "Items" in data and "DBInstance" in data["Items"]:
                for instance in data["Items"]["DBInstance"]:
                    instances.append(
                        {
                            "InstanceId": instance["DBInstanceId"],
                            "DBInstanceDescription": instance.get("DBInstanceDescription", ""),
                            "DBInstanceType": instance.get("DBInstanceType", ""),
                            "Engine": instance.get("Engine", ""),
                            "EngineVersion": instance.get("EngineVersion", ""),
                            "DBInstanceClass": instance.get("DBInstanceClass", ""),
                            "DBInstanceStatus": instance.get("DBInstanceStatus", ""),
                            "CreateTime": instance.get("CreateTime", ""),
                            "ExpireTime": instance.get("ExpireTime", ""),
                            "Region": region_id,
                        }
                    )

            return instances
        except Exception as e:
            error = ErrorHandler.handle_api_error(e, "RDS", region_id)
            ErrorHandler.handle_region_error(e, region_id, "RDS")
            return []

    def get_metrics(self, region: str, instance_id: str, days: int = 14):
        """è·å–RDSå®ä¾‹çš„ç›‘æ§æ•°æ® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_rds_metrics(region, instance_id)

    def get_rds_metrics(self, region_id, instance_id):
        """è·å–RDSå®ä¾‹çš„ç›‘æ§æ•°æ®"""
        client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
        end_time = int(round(time.time() * 1000))
        start_time = end_time - 14 * 24 * 60 * 60 * 1000  # 14å¤©å‰

        # RDSç›‘æ§æŒ‡æ ‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°ï¼‰
        metrics = {
            "CpuUsage": "CPUåˆ©ç”¨ç‡",
            "MemoryUsage": "å†…å­˜åˆ©ç”¨ç‡",
            "ConnectionUsage": "è¿æ¥æ•°ä½¿ç”¨ç‡",
            "MySQL_QPS": "æ¯ç§’æŸ¥è¯¢æ•°",
            "MySQL_TPS": "æ¯ç§’äº‹åŠ¡æ•°",
            "MySQL_ComSelect": "SELECTæŸ¥è¯¢æ•°",
            "MySQL_ComInsert": "INSERTæ“ä½œæ•°",
            "MySQL_ComUpdate": "UPDATEæ“ä½œæ•°",
            "MySQL_ComDelete": "DELETEæ“ä½œæ•°",
            "MySQL_ThreadsConnected": "è¿æ¥çº¿ç¨‹æ•°",
            "MySQL_ThreadsRunning": "è¿è¡Œçº¿ç¨‹æ•°",
            "MySQL_SlowQueries": "æ…¢æŸ¥è¯¢æ•°",
            "MySQL_OpenFiles": "æ‰“å¼€æ–‡ä»¶æ•°",
            "MySQL_OpenTables": "æ‰“å¼€è¡¨æ•°",
            "MySQL_SelectScan": "æ‰«ææŸ¥è¯¢æ•°",
        }

        result = {}

        for metric_name, display_name in metrics.items():
            try:
                request = CommonRequest()
                request.set_domain(f"cms.{region_id}.aliyuncs.com")
                request.set_method("POST")
                request.set_version("2019-01-01")
                request.set_action_name("DescribeMetricData")
                request.add_query_param("RegionId", region_id)
                request.add_query_param("Namespace", "acs_rds_dashboard")
                request.add_query_param("MetricName", metric_name)
                request.add_query_param("StartTime", start_time)
                request.add_query_param("EndTime", end_time)
                request.add_query_param("Period", "86400")  # 1å¤©èšåˆ
                request.add_query_param("Dimensions", f'[{{"instanceId":"{instance_id}"}}]')

                response = client.do_action_with_exception(request)
                data = json.loads(response)

                if "Datapoints" in data and data["Datapoints"]:
                    if isinstance(data["Datapoints"], str):
                        dps = json.loads(data["Datapoints"])
                    else:
                        dps = data["Datapoints"]

                    if dps and len(dps) > 0:
                        # è®¡ç®—æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡å€¼
                        total = 0
                        count = 0
                        for dp in dps:
                            if "Average" in dp and dp["Average"] is not None:
                                total += float(dp["Average"])
                                count += 1

                        if count > 0:
                            result[display_name] = total / count
                        else:
                            result[display_name] = 0
                    else:
                        result[display_name] = 0
                else:
                    result[display_name] = 0
            except Exception as e:
                result[display_name] = 0

            time.sleep(0.1)  # é¿å…APIé™æµ

        return result

    def save_rds_data(self, instances_data, monitoring_data):
        """ä¿å­˜RDSæ•°æ®åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨ç»Ÿä¸€DatabaseManagerï¼‰"""
        # è½¬æ¢å®ä¾‹æ•°æ®æ ¼å¼
        for instance in instances_data:
            instance_dict = {
                "InstanceId": instance["InstanceId"],
                "InstanceName": instance.get("DBInstanceDescription", ""),
                "InstanceType": instance.get("DBInstanceType", ""),
                "Region": instance.get("Region", ""),
                "Status": instance.get("DBInstanceStatus", ""),
                "CreationTime": instance.get("CreateTime", ""),
                "ExpireTime": instance.get("ExpireTime", ""),
                "engine": instance.get("Engine", ""),
                "engine_version": instance.get("EngineVersion", ""),
                "instance_class": instance.get("DBInstanceClass", ""),
            }
            self.db_manager.save_instance("rds", instance_dict)

        # ä¿å­˜ç›‘æ§æ•°æ®
        for instance_id, metrics in monitoring_data.items():
            self.db_manager.save_metrics_batch("rds", instance_id, metrics)

        self.logger.info(f"RDSæ•°æ®ä¿å­˜å®Œæˆ: {len(instances_data)}ä¸ªå®ä¾‹")

    def is_idle(self, instance, metrics, thresholds=None):
        """åˆ¤æ–­RDSå®ä¾‹æ˜¯å¦é—²ç½® (BaseResourceAnalyzeræ¥å£)"""
        is_idle = self.is_rds_idle(metrics)
        conditions = []
        if is_idle:
            conditions = [self.get_idle_reason(metrics)]
        return is_idle, conditions

    def get_optimization_suggestions(self, instance, metrics):
        """è·å–ä¼˜åŒ–å»ºè®® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_optimization_suggestion(metrics, instance.get("DBInstanceClass", ""))

    def is_rds_idle(self, metrics):
        """åˆ¤æ–­RDSå®ä¾‹æ˜¯å¦é—²ç½®"""
        # RDSé—²ç½®åˆ¤æ–­æ ‡å‡†ï¼ˆæˆ–å…³ç³»ï¼‰
        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        connection_usage = metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0)
        qps = metrics.get("æ¯ç§’æŸ¥è¯¢æ•°", 0)
        tps = metrics.get("æ¯ç§’äº‹åŠ¡æ•°", 0)
        threads_connected = metrics.get("è¿æ¥çº¿ç¨‹æ•°", 0)
        threads_running = metrics.get("è¿è¡Œçº¿ç¨‹æ•°", 0)

        # é—²ç½®æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³åˆ¤å®šä¸ºé—²ç½®ï¼‰
        idle_conditions = [
            cpu_util < 10,  # CPUåˆ©ç”¨ç‡ä½äº10%
            memory_util < 20,  # å†…å­˜åˆ©ç”¨ç‡ä½äº20%
            connection_usage < 20,  # è¿æ¥æ•°ä½¿ç”¨ç‡ä½äº20%
            qps < 100,  # QPSä½äº100
            tps < 10,  # TPSä½äº10
            threads_connected < 10,  # è¿æ¥çº¿ç¨‹æ•°ä½äº10
            threads_running < 5,  # è¿è¡Œçº¿ç¨‹æ•°ä½äº5
        ]

        return any(idle_conditions)

    def get_idle_reason(self, metrics):
        """è·å–é—²ç½®åŸå› """
        reasons = []

        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        connection_usage = metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0)
        qps = metrics.get("æ¯ç§’æŸ¥è¯¢æ•°", 0)
        tps = metrics.get("æ¯ç§’äº‹åŠ¡æ•°", 0)
        threads_connected = metrics.get("è¿æ¥çº¿ç¨‹æ•°", 0)
        threads_running = metrics.get("è¿è¡Œçº¿ç¨‹æ•°", 0)

        if cpu_util < 10:
            reasons.append(f"CPUåˆ©ç”¨ç‡({cpu_util:.1f}%) < 10%")
        if memory_util < 20:
            reasons.append(f"å†…å­˜åˆ©ç”¨ç‡({memory_util:.1f}%) < 20%")
        if connection_usage < 20:
            reasons.append(f"è¿æ¥æ•°ä½¿ç”¨ç‡({connection_usage:.1f}%) < 20%")
        if qps < 100:
            reasons.append(f"QPS({qps:.0f}) < 100")
        if tps < 10:
            reasons.append(f"TPS({tps:.0f}) < 10")
        if threads_connected < 10:
            reasons.append(f"è¿æ¥çº¿ç¨‹æ•°({threads_connected:.0f}) < 10")
        if threads_running < 5:
            reasons.append(f"è¿è¡Œçº¿ç¨‹æ•°({threads_running:.0f}) < 5")

        return "; ".join(reasons)

    def get_optimization_suggestion(self, metrics, instance_class):
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        connection_usage = metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0)
        qps = metrics.get("æ¯ç§’æŸ¥è¯¢æ•°", 0)
        tps = metrics.get("æ¯ç§’äº‹åŠ¡æ•°", 0)

        if cpu_util < 10 and memory_util < 20:
            suggestions.append("å»ºè®®é™é…å®ä¾‹è§„æ ¼")
        elif cpu_util < 10:
            suggestions.append("å»ºè®®é™é…CPUè§„æ ¼")
        elif memory_util < 20:
            suggestions.append("å»ºè®®é™é…å†…å­˜è§„æ ¼")

        if connection_usage < 20:
            suggestions.append("å»ºè®®å‡å°‘æœ€å¤§è¿æ¥æ•°")

        if qps < 100 and tps < 10:
            suggestions.append("å»ºè®®ä½¿ç”¨æ›´å°çš„å®ä¾‹ç±»å‹")

        return "; ".join(suggestions) if suggestions else "å»ºè®®ä¿æŒå½“å‰é…ç½®"

    def get_monthly_cost(self, instance_id, instance_class, region):
        """è·å–RDSå®ä¾‹æœˆæˆæœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦è°ƒç”¨BSS APIï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨DescribeRenewalPrice API
        # æ ¹æ®å®ä¾‹ç±»å‹å’ŒåŒºåŸŸè¿”å›ä¼°ç®—æˆæœ¬
        cost_map = {
            "rds.mysql.s1.small": 200,
            "rds.mysql.s2.small": 300,
            "rds.mysql.s1.medium": 400,
            "rds.mysql.s2.medium": 600,
            "rds.mysql.s1.large": 800,
            "rds.mysql.s2.large": 1200,
            "rds.mysql.s1.xlarge": 1600,
            "rds.mysql.s2.xlarge": 2400,
        }

        return cost_map.get(instance_class, 500)  # é»˜è®¤500å…ƒ

    def analyze(self, regions=None, days=14):
        """åˆ†æèµ„æº (BaseResourceAnalyzeræ¥å£)"""
        # å¤ç”¨ç°æœ‰çš„analyze_rds_resourcesé€»è¾‘ï¼Œä½†éœ€è¦é€‚é…è¿”å›æ ¼å¼
        # è¿™é‡Œç®€å•èµ·è§ï¼Œç›´æ¥è°ƒç”¨analyze_rds_resourcesï¼Œå®ƒè¿”å›çš„æ˜¯idle_instancesåˆ—è¡¨
        # ä½†BaseResourceAnalyzer.analyzeé€šå¸¸è¿”å›åŒ…å«metricsç­‰çš„è¯¦ç»†å­—å…¸åˆ—è¡¨
        # ç”±äºRDSAnalyzerçš„analyze_rds_resourceså·²ç»åšäº†å¾ˆå¤šå·¥ä½œï¼Œæˆ‘ä»¬è¿™é‡Œæš‚æ—¶ä¿ç•™å®ƒçš„é€»è¾‘
        # å¹¶è®©analyzeè¿”å›å®ƒè¿”å›çš„ç»“æœï¼ˆè™½ç„¶ç±»å‹å¯èƒ½ä¸å¤ªåŒ¹é…BaseResourceAnalyzerçš„ç­¾åï¼Œä½†åœ¨åŠ¨æ€è¯­è¨€ä¸­æ˜¯å¯ä»¥çš„ï¼‰
        return self.analyze_rds_resources()

    def generate_report(self, idle_instances):
        """ç”ŸæˆæŠ¥å‘Š (BaseResourceAnalyzeræ¥å£)"""
        self.generate_rds_report(idle_instances)

    def analyze_rds_resources(self):
        """åˆ†æRDSèµ„æº"""
        self.logger.info("å¼€å§‹RDSèµ„æºåˆ†æ...")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()

        # è·å–æ‰€æœ‰åŒºåŸŸ
        regions = self.get_all_regions()
        self.logger.info(f"è·å–åˆ° {len(regions)} ä¸ªåŒºåŸŸ")

        # å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„å®ä¾‹
        self.logger.info("å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„RDSå®ä¾‹...")

        def get_region_instances(region_item):
            """è·å–å•ä¸ªåŒºåŸŸçš„å®ä¾‹ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
            region = region_item
            try:
                instances = self.get_rds_instances(region)
                return {"region": region, "instances": instances}
            except Exception as e:
                self.logger.warning(f"åŒºåŸŸ {region} è·å–å®ä¾‹å¤±è´¥: {e}")
                return {"region": region, "instances": []}

        # å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„å®ä¾‹
        region_results = process_concurrently(
            regions, get_region_instances, max_workers=10, description="è·å–RDSå®ä¾‹"
        )

        # æ•´ç†æ‰€æœ‰å®ä¾‹
        all_instances = []
        for result in region_results:
            if result and result.get("instances"):
                all_instances.extend(result["instances"])
                self.logger.info(f"{result['region']}: {len(result['instances'])} ä¸ªå®ä¾‹")

        if not all_instances:
            self.logger.warning("æœªå‘ç°ä»»ä½•RDSå®ä¾‹")
            return []

        self.logger.info(f"æ€»å…±è·å–åˆ° {len(all_instances)} ä¸ªRDSå®ä¾‹")

        # å®šä¹‰å•ä¸ªå®ä¾‹å¤„ç†å‡½æ•°ï¼ˆç”¨äºå¹¶å‘ï¼‰
        def process_single_instance(instance_item):
            """å¤„ç†å•ä¸ªå®ä¾‹ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
            instance = instance_item
            instance_id = instance["InstanceId"]
            region = instance["Region"]

            try:
                metrics = self.get_rds_metrics(region, instance_id)
                return {"success": True, "instance_id": instance_id, "metrics": metrics}
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "RDS", region, instance_id)
                ErrorHandler.handle_instance_error(
                    e, instance_id, region, "RDS", continue_on_error=True
                )
                return {
                    "success": False,
                    "instance_id": instance_id,
                    "metrics": {},
                    "error": str(e),
                }

        # å¹¶å‘è·å–ç›‘æ§æ•°æ®
        self.logger.info("å¹¶å‘è·å–ç›‘æ§æ•°æ®ï¼ˆæœ€å¤š10ä¸ªå¹¶å‘çº¿ç¨‹ï¼‰...")

        def progress_callback(completed, total):
            progress_pct = completed / total * 100
            sys.stdout.write(f"\rğŸ“Š ç›‘æ§æ•°æ®è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%)")
            sys.stdout.flush()

        monitoring_results = process_concurrently(
            all_instances,
            process_single_instance,
            max_workers=10,
            description="RDSç›‘æ§æ•°æ®é‡‡é›†",
            progress_callback=progress_callback,
        )

        # æ•´ç†ç›‘æ§æ•°æ®
        all_monitoring_data = {}
        success_count = 0
        fail_count = 0

        for result in monitoring_results:
            if result and result.get("success"):
                all_monitoring_data[result["instance_id"]] = result["metrics"]
                success_count += 1
            else:
                if result:
                    instance_id = result.get("instance_id", "unknown")
                    all_monitoring_data[instance_id] = {}
                    fail_count += 1

        self.logger.info(f"ç›‘æ§æ•°æ®è·å–å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")

        # ä¿å­˜æ•°æ®
        self.save_rds_data(all_instances, all_monitoring_data)

        # åˆ†æé—²ç½®å®ä¾‹
        idle_instances = []
        for instance in all_instances:
            instance_id = instance["InstanceId"]
            metrics = all_monitoring_data.get(instance_id, {})

            if self.is_rds_idle(metrics):
                idle_reason = self.get_idle_reason(metrics)
                optimization = self.get_optimization_suggestion(
                    metrics, instance["DBInstanceClass"]
                )
                monthly_cost = self.get_monthly_cost(
                    instance_id, instance["DBInstanceClass"], instance["Region"]
                )

                idle_instances.append(
                    {
                        "å®ä¾‹ID": instance_id,
                        "å®ä¾‹åç§°": instance["DBInstanceDescription"],
                        "å®ä¾‹ç±»å‹": instance["DBInstanceClass"],
                        "å¼•æ“": instance["Engine"],
                        "ç‰ˆæœ¬": instance["EngineVersion"],
                        "åŒºåŸŸ": instance["Region"],
                        "çŠ¶æ€": instance["DBInstanceStatus"],
                        "CPUåˆ©ç”¨ç‡(%)": metrics.get("CPUåˆ©ç”¨ç‡", 0),
                        "å†…å­˜åˆ©ç”¨ç‡(%)": metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0),
                        "è¿æ¥æ•°ä½¿ç”¨ç‡(%)": metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0),
                        "QPS": metrics.get("æ¯ç§’æŸ¥è¯¢æ•°", 0),
                        "TPS": metrics.get("æ¯ç§’äº‹åŠ¡æ•°", 0),
                        "è¿æ¥çº¿ç¨‹æ•°": metrics.get("è¿æ¥çº¿ç¨‹æ•°", 0),
                        "è¿è¡Œçº¿ç¨‹æ•°": metrics.get("è¿è¡Œçº¿ç¨‹æ•°", 0),
                        "æ…¢æŸ¥è¯¢æ•°": metrics.get("æ…¢æŸ¥è¯¢æ•°", 0),
                        "æ‰“å¼€æ–‡ä»¶æ•°": metrics.get("æ‰“å¼€æ–‡ä»¶æ•°", 0),
                        "æ‰“å¼€è¡¨æ•°": metrics.get("æ‰“å¼€è¡¨æ•°", 0),
                        "SELECTæŸ¥è¯¢æ•°": metrics.get("SELECTæŸ¥è¯¢æ•°", 0),
                        "INSERTæ“ä½œæ•°": metrics.get("INSERTæ“ä½œæ•°", 0),
                        "UPDATEæ“ä½œæ•°": metrics.get("UPDATEæ“ä½œæ•°", 0),
                        "DELETEæ“ä½œæ•°": metrics.get("DELETEæ“ä½œæ•°", 0),
                        "é—²ç½®åŸå› ": idle_reason,
                        "ä¼˜åŒ–å»ºè®®": optimization,
                        "æœˆæˆæœ¬(Â¥)": monthly_cost,
                    }
                )

        self.logger.info(f"RDSåˆ†æå®Œæˆ: å‘ç° {len(idle_instances)} ä¸ªé—²ç½®å®ä¾‹")
        return idle_instances

    def generate_rds_report(self, idle_instances):
        """ç”ŸæˆRDSæŠ¥å‘Šï¼ˆä½¿ç”¨ç»Ÿä¸€ReportGeneratorï¼‰"""
        if not idle_instances:
            self.logger.info("æ²¡æœ‰å‘ç°é—²ç½®çš„RDSå®ä¾‹")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä½¿ç”¨ç»Ÿä¸€æŠ¥å‘Šç”Ÿæˆå™¨
        reports = ReportGenerator.generate_combined_report(
            resource_type="RDS",
            idle_instances=idle_instances,
            output_dir=".",
            tenant_name=self.tenant_name,
            timestamp=timestamp,
        )

        self.logger.info(f"ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {reports['excel']}")
        self.logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {reports['html']}")

        # ç»Ÿè®¡ä¿¡æ¯
        total_cost = sum(instance.get("æœˆæˆæœ¬(Â¥)", 0) for instance in idle_instances)
        self.logger.info(
            f"RDSé—²ç½®å®ä¾‹ç»Ÿè®¡: æ€»æ•°é‡={len(idle_instances)}ä¸ª, æ€»æœˆæˆæœ¬={total_cost:,.2f}å…ƒ, é¢„è®¡å¹´èŠ‚çœ={total_cost * 12:,.2f}å…ƒ"
        )

    # generate_html_reportæ–¹æ³•å·²ç§»é™¤ï¼Œæ”¹ç”¨ReportGenerator.generate_combined_report


def main():
    """RDSåˆ†æä¸»å‡½æ•°"""
    # è¯»å–é…ç½®æ–‡ä»¶
    try:
        with open("config.json", "r") as f:
            config = json.load(f)
        access_key_id = config["access_key_id"]
        access_key_secret = config["access_key_secret"]
    except FileNotFoundError:
        import logging

        logging.error("é…ç½®æ–‡ä»¶ config.json ä¸å­˜åœ¨")
        return

    # åˆ›å»ºRDSåˆ†æå™¨
    analyzer = RDSAnalyzer(access_key_id, access_key_secret)

    # åˆ†æRDSèµ„æº
    idle_instances = analyzer.analyze_rds_resources()

    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_rds_report(idle_instances)


if __name__ == "__main__":
    main()
