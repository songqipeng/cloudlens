#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PolarDBèµ„æºåˆ†ææ¨¡å—
åˆ†æé˜¿é‡Œäº‘PolarDBäº‘åŸç”Ÿæ•°æ®åº“çš„é—²ç½®æƒ…å†µ
"""

import json
import sys
import time
from datetime import datetime

from aliyunsdkcore.client import AcsClient
from aliyunsdkcore.request import CommonRequest

from cloudlens.core.analyzer_registry import AnalyzerRegistry
from cloudlens.core.base_analyzer import BaseResourceAnalyzer
from cloudlens.core.db_manager import DatabaseManager
from cloudlens.core.report_generator import ReportGenerator
from cloudlens.utils.concurrent_helper import process_concurrently
from cloudlens.utils.error_handler import ErrorHandler
from cloudlens.utils.logger import get_logger


@AnalyzerRegistry.register("polardb", "PolarDBäº‘åŸç”Ÿæ•°æ®åº“", "ğŸ¬")
class PolarDBAnalyzer(BaseResourceAnalyzer):
    """PolarDBèµ„æºåˆ†æå™¨"""

    def __init__(self, access_key_id, access_key_secret, tenant_name=None):
        super().__init__(
            access_key_id=access_key_id,
            access_key_secret=access_key_secret,
            tenant_name=tenant_name or "default",
        )
        self.db_name = "polardb_monitoring_data.db"
        self.logger = get_logger("polardb_analyzer")
        self.db_manager = DatabaseManager(self.db_name)

    def get_resource_type(self) -> str:
        return "polardb"

    def init_database(self):
        """åˆå§‹åŒ–PolarDBæ•°æ®åº“"""
        extra_columns = {"engine": "TEXT", "engine_version": "TEXT", "db_node_class": "TEXT"}
        self.db_manager.create_resource_table("polardb", extra_columns)
        self.db_manager.create_monitoring_table("polardb")
        self.logger.info("PolarDBæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    def get_all_regions(self):
        """è·å–æ‰€æœ‰å¯ç”¨åŒºåŸŸ"""
        client = AcsClient(self.access_key_id, self.access_key_secret, "cn-hangzhou")
        request = CommonRequest()
        request.set_domain("ecs.cn-hangzhou.aliyuncs.com")
        request.set_method("POST")
        request.set_version("2014-05-26")
        request.set_action_name("DescribeRegions")
        response = client.do_action_with_exception(request)
        data = json.loads(response)
        return [region["RegionId"] for region in data["Regions"]["Region"]]

    def get_instances(self, region: str):
        """è·å–æŒ‡å®šåŒºåŸŸçš„PolarDBé›†ç¾¤ (BaseResourceAnalyzeræ¥å£)"""
        return self.get_polardb_clusters(region)

    def get_polardb_clusters(self, region_id):
        """è·å–æŒ‡å®šåŒºåŸŸçš„PolarDBé›†ç¾¤"""
        try:
            client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
            request = CommonRequest()
            request.set_domain(f"polardb.{region_id}.aliyuncs.com")
            request.set_method("POST")
            request.set_version("2017-08-01")
            request.set_action_name("DescribeDBClusters")
            request.add_query_param("PageSize", 100)
            request.add_query_param("PageNumber", 1)

            all_clusters = []
            page_number = 1

            while True:
                request.add_query_param("PageNumber", page_number)
                response = client.do_action_with_exception(request)
                data = json.loads(response)

                if "Items" in data and "DBCluster" in data["Items"]:
                    clusters = data["Items"]["DBCluster"]
                    if not isinstance(clusters, list):
                        clusters = [clusters]

                    if len(clusters) == 0:
                        break

                    for cluster in clusters:
                        all_clusters.append(
                            {
                                "DBClusterId": cluster.get("DBClusterId", ""),
                                "DBClusterDescription": cluster.get("DBClusterDescription", ""),
                                "Engine": cluster.get("Engine", ""),
                                "DBVersion": cluster.get("DBVersion", ""),
                                "DBNodeClass": cluster.get("DBNodeClass", ""),
                                "RegionId": cluster.get("RegionId", region_id),
                                "DBClusterStatus": cluster.get("DBClusterStatus", ""),
                                "CreateTime": cluster.get("CreateTime", ""),
                            }
                        )

                    page_number += 1
                    if len(clusters) < 100:
                        break
                else:
                    break

            return all_clusters
        except Exception as e:
            error = ErrorHandler.handle_api_error(e, "PolarDB", region_id)
            ErrorHandler.handle_region_error(e, region_id, "PolarDB")
            return []

    def get_metrics(self, region: str, instance_id: str, days: int = 14):
        """è·å–PolarDBé›†ç¾¤çš„ç›‘æ§æ•°æ® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_polardb_metrics(region, instance_id)

    def get_polardb_metrics(self, region_id, cluster_id):
        """è·å–PolarDBé›†ç¾¤çš„ç›‘æ§æ•°æ®"""
        client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
        end_time = int(round(time.time() * 1000))
        start_time = end_time - 14 * 24 * 60 * 60 * 1000

        metrics = {
            "CPUUtilization": "CPUåˆ©ç”¨ç‡",
            "MemoryUtilization": "å†…å­˜åˆ©ç”¨ç‡",
            "ConnectionUsage": "è¿æ¥æ•°ä½¿ç”¨ç‡",
            "IOPSUsage": "IOPSä½¿ç”¨ç‡",
            "DiskUsage": "ç£ç›˜ä½¿ç”¨ç‡",
        }

        result = {}

        for metric_name, display_name in metrics.items():
            try:
                request = CommonRequest()
                request.set_domain(f"cms.{region_id}.aliyuncs.com")
                request.set_method("POST")
                request.set_version("2019-01-01")
                request.set_action_name("DescribeMetricData")
                request.add_query_param("RegionId", region_id)
                request.add_query_param("Namespace", "acs_polardb")
                request.add_query_param("MetricName", metric_name)
                request.add_query_param("StartTime", start_time)
                request.add_query_param("EndTime", end_time)
                request.add_query_param("Period", "86400")
                request.add_query_param("Dimensions", f'[{{"dbClusterId":"{cluster_id}"}}]')

                response = client.do_action_with_exception(request)
                data = json.loads(response)

                if "Datapoints" in data and data["Datapoints"]:
                    if isinstance(data["Datapoints"], str):
                        dps = json.loads(data["Datapoints"])
                    else:
                        dps = data["Datapoints"]

                    if dps:
                        values = [float(dp.get("Average", 0)) for dp in dps if "Average" in dp]
                        if values:
                            result[display_name] = sum(values) / len(values)
                        else:
                            result[display_name] = 0
                    else:
                        result[display_name] = 0
                else:
                    result[display_name] = 0

                time.sleep(0.1)
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "PolarDB", region_id, cluster_id)
                result[display_name] = 0

        return result

    def save_polardb_data(self, clusters_data, monitoring_data):
        """ä¿å­˜PolarDBæ•°æ®åˆ°æ•°æ®åº“"""
        for cluster in clusters_data:
            instance_dict = {
                "InstanceId": cluster.get("DBClusterId", ""),
                "InstanceName": cluster.get("DBClusterDescription", ""),
                "InstanceType": cluster.get("DBNodeClass", ""),
                "Region": cluster.get("RegionId", ""),
                "Status": cluster.get("DBClusterStatus", ""),
                "CreationTime": cluster.get("CreateTime", ""),
                "engine": cluster.get("Engine", ""),
                "engine_version": cluster.get("DBVersion", ""),
                "db_node_class": cluster.get("DBNodeClass", ""),
            }
            self.db_manager.save_instance("polardb", instance_dict)

        for cluster_id, metrics in monitoring_data.items():
            self.db_manager.save_metrics_batch("polardb", cluster_id, metrics)

        self.logger.info(f"PolarDBæ•°æ®ä¿å­˜å®Œæˆ: {len(clusters_data)}ä¸ªé›†ç¾¤")

    def is_idle(self, instance, metrics, thresholds=None):
        """åˆ¤æ–­PolarDBé›†ç¾¤æ˜¯å¦é—²ç½® (BaseResourceAnalyzeræ¥å£)"""
        is_idle = self.is_polardb_idle(metrics)
        conditions = []
        if is_idle:
            conditions = [self.get_idle_reason(metrics)]
        return is_idle, conditions

    def get_optimization_suggestions(self, instance, metrics):
        """è·å–ä¼˜åŒ–å»ºè®® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_optimization_suggestion(metrics, instance.get("DBNodeClass", ""))

    def is_polardb_idle(self, metrics):
        """åˆ¤æ–­PolarDBé›†ç¾¤æ˜¯å¦é—²ç½®"""
        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        connection_util = metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0)
        iops_util = metrics.get("IOPSä½¿ç”¨ç‡", 0)

        # PolarDBé—²ç½®åˆ¤æ–­æ ‡å‡†ï¼ˆæˆ–å…³ç³»ï¼‰
        if cpu_util < 10:
            return True
        if memory_util < 20:
            return True
        if connection_util < 20:
            return True
        if iops_util < 10:
            return True

        return False

    def get_idle_reason(self, metrics):
        """è·å–é—²ç½®åŸå› """
        reasons = []
        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        connection_util = metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0)

        if cpu_util < 10:
            reasons.append(f"CPUåˆ©ç”¨ç‡ {cpu_util:.2f}% < 10%")
        if memory_util < 20:
            reasons.append(f"å†…å­˜åˆ©ç”¨ç‡ {memory_util:.2f}% < 20%")
        if connection_util < 20:
            reasons.append(f"è¿æ¥æ•°ä½¿ç”¨ç‡ {connection_util:.2f}% < 20%")

        return ", ".join(reasons) if reasons else "æœªæ»¡è¶³é—²ç½®æ¡ä»¶"

    def get_optimization_suggestion(self, metrics, db_node_class):
        """è·å–ä¼˜åŒ–å»ºè®®"""
        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)

        if cpu_util < 5:
            return "è€ƒè™‘åˆ é™¤é›†ç¾¤ï¼ˆä½¿ç”¨ç‡æä½ï¼‰"
        elif cpu_util < 15:
            return "è€ƒè™‘é™é…èŠ‚ç‚¹è§„æ ¼"
        else:
            return "æŒç»­ç›‘æ§ï¼Œè€ƒè™‘ä¼˜åŒ–é…ç½®"

    def get_monthly_cost(self, cluster_id, db_node_class, region):
        """è·å–æœˆåº¦æˆæœ¬ä¼°ç®—"""
        return 800.0  # é»˜è®¤800å…ƒ/æœˆ

    def analyze(self, regions=None, days=14):
        """åˆ†æèµ„æº (BaseResourceAnalyzeræ¥å£)"""
        # PolarDBAnalyzerçš„analyze_polardb_resourcesé€»è¾‘æ¯”è¾ƒå¤æ‚
        return self.analyze_polardb_resources()

    def generate_report(self, idle_instances):
        """ç”ŸæˆæŠ¥å‘Š (BaseResourceAnalyzeræ¥å£)"""
        self.generate_polardb_report(idle_instances)

    def analyze_polardb_resources(self):
        """åˆ†æPolarDBèµ„æº"""
        self.init_database()
        self.logger.info("å¼€å§‹PolarDBèµ„æºåˆ†æ...")

        all_regions = self.get_all_regions()
        all_clusters = []

        def get_region_clusters(region_item):
            region_id = region_item
            try:
                clusters = self.get_polardb_clusters(region_id)
                return {"success": True, "region": region_id, "clusters": clusters}
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "PolarDB", region_id)
                return {"success": False, "region": region_id, "clusters": []}

        results = process_concurrently(
            all_regions, get_region_clusters, max_workers=10, description="PolarDBé›†ç¾¤é‡‡é›†"
        )

        for result in results:
            if result and result.get("success"):
                all_clusters.extend(result["clusters"])

        self.logger.info(f"æ€»å…±è·å–åˆ° {len(all_clusters)} ä¸ªPolarDBé›†ç¾¤")

        def process_single_cluster(cluster_item):
            cluster = cluster_item
            cluster_id = cluster["DBClusterId"]
            region = cluster["RegionId"]

            try:
                metrics = self.get_polardb_metrics(region, cluster_id)
                return {"success": True, "cluster_id": cluster_id, "metrics": metrics}
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "PolarDB", region, cluster_id)
                return {"success": False, "cluster_id": cluster_id, "metrics": {}, "error": str(e)}

        self.logger.info("å¹¶å‘è·å–ç›‘æ§æ•°æ®...")
        monitoring_results = process_concurrently(
            all_clusters, process_single_cluster, max_workers=10, description="PolarDBç›‘æ§æ•°æ®é‡‡é›†"
        )

        all_monitoring_data = {}
        for result in monitoring_results:
            if result and result.get("success"):
                all_monitoring_data[result["cluster_id"]] = result["metrics"]

        self.save_polardb_data(all_clusters, all_monitoring_data)

        idle_clusters = []
        for cluster in all_clusters:
            cluster_id = cluster["DBClusterId"]
            metrics = all_monitoring_data.get(cluster_id, {})

            if self.is_polardb_idle(metrics):
                idle_reason = self.get_idle_reason(metrics)
                optimization = self.get_optimization_suggestion(
                    metrics, cluster.get("DBNodeClass", "")
                )
                monthly_cost = self.get_monthly_cost(
                    cluster_id, cluster.get("DBNodeClass", ""), cluster["RegionId"]
                )

                idle_clusters.append(
                    {
                        "é›†ç¾¤ID": cluster_id,
                        "é›†ç¾¤åç§°": cluster.get("DBClusterDescription", ""),
                        "å¼•æ“": cluster.get("Engine", ""),
                        "ç‰ˆæœ¬": cluster.get("DBVersion", ""),
                        "èŠ‚ç‚¹è§„æ ¼": cluster.get("DBNodeClass", ""),
                        "åŒºåŸŸ": cluster["RegionId"],
                        "çŠ¶æ€": cluster.get("DBClusterStatus", ""),
                        "CPUåˆ©ç”¨ç‡(%)": metrics.get("CPUåˆ©ç”¨ç‡", 0),
                        "å†…å­˜åˆ©ç”¨ç‡(%)": metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0),
                        "è¿æ¥æ•°ä½¿ç”¨ç‡(%)": metrics.get("è¿æ¥æ•°ä½¿ç”¨ç‡", 0),
                        "IOPSä½¿ç”¨ç‡(%)": metrics.get("IOPSä½¿ç”¨ç‡", 0),
                        "é—²ç½®åŸå› ": idle_reason,
                        "ä¼˜åŒ–å»ºè®®": optimization,
                        "æœˆæˆæœ¬(Â¥)": monthly_cost,
                    }
                )

        self.logger.info(f"PolarDBåˆ†æå®Œæˆ: å‘ç° {len(idle_clusters)} ä¸ªé—²ç½®é›†ç¾¤")
        return idle_clusters

    def generate_polardb_report(self, idle_clusters):
        """ç”ŸæˆPolarDBæŠ¥å‘Š"""
        if not idle_clusters:
            self.logger.info("æ²¡æœ‰å‘ç°é—²ç½®çš„PolarDBé›†ç¾¤")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        reports = ReportGenerator.generate_combined_report(
            resource_type="PolarDB",
            idle_instances=idle_clusters,
            output_dir=".",
            tenant_name=self.tenant_name,
            timestamp=timestamp,
        )

        self.logger.info(f"ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {reports['excel']}")
        self.logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {reports['html']}")


def main():
    """PolarDBåˆ†æä¸»å‡½æ•°"""
    try:
        with open("config.json", "r") as f:
            config = json.load(f)
        access_key_id = config["access_key_id"]
        access_key_secret = config["access_key_secret"]
    except FileNotFoundError:
        import logging

        logging.error("é…ç½®æ–‡ä»¶ config.json ä¸å­˜åœ¨")
        return

    analyzer = PolarDBAnalyzer(access_key_id, access_key_secret)
    idle_clusters = analyzer.analyze_polardb_resources()
    analyzer.generate_polardb_report(idle_clusters)


if __name__ == "__main__":
    main()
