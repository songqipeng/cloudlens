#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ClickHouseèµ„æºåˆ†ææ¨¡å—
åˆ†æé˜¿é‡Œäº‘ClickHouseå®ä¾‹çš„é—²ç½®æƒ…å†µï¼Œæä¾›ä¼˜åŒ–å»ºè®®
"""

import json
import sqlite3
import sys
import time
from datetime import datetime

import pandas as pd
from aliyunsdkcore.client import AcsClient
from aliyunsdkcore.request import CommonRequest

from cloudlens.core.analyzer_registry import AnalyzerRegistry
from cloudlens.core.base_analyzer import BaseResourceAnalyzer
from cloudlens.core.report_generator import ReportGenerator
from cloudlens.utils.concurrent_helper import process_concurrently
from cloudlens.utils.error_handler import ErrorHandler
from cloudlens.utils.logger import get_logger


@AnalyzerRegistry.register("clickhouse", "ClickHouseæ•°æ®ä»“åº“", "ğŸ“Š")
class ClickHouseAnalyzer(BaseResourceAnalyzer):
    """ClickHouseèµ„æºåˆ†æå™¨"""

    def __init__(self, access_key_id, access_key_secret, tenant_name=None):
        super().__init__(
            access_key_id=access_key_id,
            access_key_secret=access_key_secret,
            tenant_name=tenant_name or "default",
        )
        self.db_name = "clickhouse_monitoring_data.db"
        self.logger = get_logger("clickhouse_analyzer")

    def get_resource_type(self) -> str:
        return "clickhouse"

    def init_database(self):
        """åˆå§‹åŒ–ClickHouseæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        # åˆ›å»ºClickHouseå®ä¾‹è¡¨
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS clickhouse_instances (
            instance_id TEXT PRIMARY KEY,
            instance_name TEXT,
            instance_type TEXT,
            engine_version TEXT,
            instance_class TEXT,
            region TEXT,
            status TEXT,
            creation_time TEXT,
            expire_time TEXT,
            monthly_cost REAL DEFAULT 0
        )
        """
        )

        # åˆ›å»ºClickHouseç›‘æ§æ•°æ®è¡¨
        cursor.execute(
            """
        CREATE TABLE IF NOT EXISTS clickhouse_monitoring_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            instance_id TEXT,
            metric_name TEXT,
            metric_value REAL,
            timestamp INTEGER,
            FOREIGN KEY (instance_id) REFERENCES clickhouse_instances (instance_id)
        )
        """
        )

        conn.commit()
        conn.close()
        self.logger.info("ClickHouseæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    def get_all_regions(self):
        """è·å–æ‰€æœ‰å¯ç”¨åŒºåŸŸ"""
        client = AcsClient(self.access_key_id, self.access_key_secret, "cn-hangzhou")
        request = CommonRequest()
        request.set_domain("ecs.cn-hangzhou.aliyuncs.com")
        request.set_method("POST")
        request.set_version("2014-05-26")
        request.set_action_name("DescribeRegions")

        response = client.do_action_with_exception(request)
        data = json.loads(response)

        regions = []
        for region in data["Regions"]["Region"]:
            regions.append(region["RegionId"])

        return regions

    def get_instances(self, region: str):
        """è·å–æŒ‡å®šåŒºåŸŸçš„ClickHouseå®ä¾‹ (BaseResourceAnalyzeræ¥å£)"""
        return self.get_clickhouse_instances(region)

    def get_clickhouse_instances(self, region_id):
        """è·å–æŒ‡å®šåŒºåŸŸçš„ClickHouseå®ä¾‹"""
        try:
            client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
            request = CommonRequest()
            request.set_domain("clickhouse.{}.aliyuncs.com".format(region_id))
            request.set_method("POST")
            request.set_version("2019-11-11")
            request.set_action_name("DescribeDBInstances")
            request.add_query_param("PageSize", 100)
            request.add_query_param("PageNumber", 1)

            # å°è¯•è·å–æ‰€æœ‰é¡µ
            all_instances = []
            page_number = 1

            while True:
                request.add_query_param("PageNumber", page_number)
                response = client.do_action_with_exception(request)
                data = json.loads(response)

                if "Items" in data and "DBInstance" in data["Items"]:
                    instances = data["Items"]["DBInstance"]
                    if not isinstance(instances, list):
                        instances = [instances]

                    if len(instances) == 0:
                        break

                    for instance in instances:
                        all_instances.append(
                            {
                                "InstanceId": instance.get("DBInstanceId", ""),
                                "DBInstanceDescription": instance.get("DBInstanceDescription", ""),
                                "DBInstanceType": instance.get("DBInstanceType", ""),
                                "EngineVersion": instance.get("EngineVersion", ""),
                                "DBInstanceClass": instance.get("DBInstanceClass", ""),
                                "DBInstanceStatus": instance.get("DBInstanceStatus", ""),
                                "CreateTime": instance.get("CreateTime", ""),
                                "ExpireTime": instance.get("ExpireTime", ""),
                                "Region": region_id,
                            }
                        )

                    total_count = data.get("TotalCount", 0)
                    if len(all_instances) >= total_count or len(instances) < 100:
                        break

                    page_number += 1
                else:
                    break

            return all_instances
        except Exception as e:
            # ClickHouseå¯èƒ½åœ¨æŸäº›åŒºåŸŸä¸å¯ç”¨ï¼Œé™é»˜å¤±è´¥
            return []

    def get_metrics(self, region: str, instance_id: str, days: int = 14):
        """è·å–ClickHouseå®ä¾‹çš„ç›‘æ§æ•°æ® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_clickhouse_metrics(region, instance_id)

    def get_clickhouse_metrics(self, region_id, instance_id):
        """è·å–ClickHouseå®ä¾‹çš„ç›‘æ§æ•°æ®"""
        client = AcsClient(self.access_key_id, self.access_key_secret, region_id)
        end_time = int(round(time.time() * 1000))
        start_time = end_time - 14 * 24 * 60 * 60 * 1000  # 14å¤©å‰

        # ClickHouseç›‘æ§æŒ‡æ ‡ï¼ˆä½¿ç”¨CMSäº‘ç›‘æ§ï¼‰
        metrics = {
            "CpuUsage": "CPUåˆ©ç”¨ç‡",
            "MemoryUsage": "å†…å­˜åˆ©ç”¨ç‡",
            "DiskUsage": "ç£ç›˜åˆ©ç”¨ç‡",
            "ConnectionCount": "è¿æ¥æ•°",
            "QueryCount": "æŸ¥è¯¢æ•°",
            "InsertCount": "æ’å…¥æ•°",
            "NetworkIn": "ç½‘ç»œå…¥æµé‡",
            "NetworkOut": "ç½‘ç»œå‡ºæµé‡",
            "DiskReadIOPS": "ç£ç›˜è¯»IOPS",
            "DiskWriteIOPS": "ç£ç›˜å†™IOPS",
        }

        result = {}

        for metric_name, display_name in metrics.items():
            try:
                request = CommonRequest()
                request.set_domain(f"cms.{region_id}.aliyuncs.com")
                request.set_method("POST")
                request.set_version("2019-01-01")
                request.set_action_name("DescribeMetricData")
                request.add_query_param("RegionId", region_id)
                request.add_query_param("Namespace", "acs_clickhouse_dashboard")
                request.add_query_param("MetricName", metric_name)
                request.add_query_param("StartTime", start_time)
                request.add_query_param("EndTime", end_time)
                request.add_query_param("Period", "86400")  # 1å¤©èšåˆ
                request.add_query_param("Dimensions", f'[{{"instanceId":"{instance_id}"}}]')

                response = client.do_action_with_exception(request)
                data = json.loads(response)

                if "Datapoints" in data and data["Datapoints"]:
                    if isinstance(data["Datapoints"], str):
                        dps = json.loads(data["Datapoints"])
                    else:
                        dps = data["Datapoints"]

                    if dps and len(dps) > 0:
                        # è®¡ç®—æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡å€¼
                        total = 0
                        count = 0
                        for dp in dps:
                            if "Average" in dp and dp["Average"] is not None:
                                total += float(dp["Average"])
                                count += 1

                        if count > 0:
                            result[display_name] = total / count
                        else:
                            result[display_name] = 0
                    else:
                        result[display_name] = 0
                else:
                    result[display_name] = 0
            except Exception as e:
                # æŸäº›æŒ‡æ ‡å¯èƒ½ä¸å¯ç”¨ï¼Œè®¾ç½®ä¸º0
                result[display_name] = 0

            time.sleep(0.1)  # é¿å…APIé™æµ

        return result

    def save_clickhouse_data(self, instances_data, monitoring_data):
        """ä¿å­˜ClickHouseæ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_name)
        cursor = conn.cursor()

        # ä¿å­˜å®ä¾‹æ•°æ®
        for instance in instances_data:
            cursor.execute(
                """
            INSERT OR REPLACE INTO clickhouse_instances 
            (instance_id, instance_name, instance_type, engine_version, 
             instance_class, region, status, creation_time, expire_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    instance["InstanceId"],
                    instance["DBInstanceDescription"],
                    instance["DBInstanceType"],
                    instance["EngineVersion"],
                    instance["DBInstanceClass"],
                    instance["Region"],
                    instance["DBInstanceStatus"],
                    instance["CreateTime"],
                    instance["ExpireTime"],
                ),
            )

        # ä¿å­˜ç›‘æ§æ•°æ®
        for instance_id, metrics in monitoring_data.items():
            for metric_name, metric_value in metrics.items():
                cursor.execute(
                    """
                INSERT INTO clickhouse_monitoring_data 
                (instance_id, metric_name, metric_value, timestamp)
                VALUES (?, ?, ?, ?)
                """,
                    (instance_id, metric_name, metric_value, int(time.time())),
                )

        conn.commit()
        conn.close()
        self.logger.info(f"ClickHouseæ•°æ®ä¿å­˜å®Œæˆ: {len(instances_data)}ä¸ªå®ä¾‹")

    def is_idle(self, instance, metrics, thresholds=None):
        """åˆ¤æ–­ClickHouseå®ä¾‹æ˜¯å¦é—²ç½® (BaseResourceAnalyzeræ¥å£)"""
        is_idle = self.is_clickhouse_idle(metrics)
        conditions = []
        if is_idle:
            conditions = [self.get_idle_reason(metrics)]
        return is_idle, conditions

    def get_optimization_suggestions(self, instance, metrics):
        """è·å–ä¼˜åŒ–å»ºè®® (BaseResourceAnalyzeræ¥å£)"""
        return self.get_optimization_suggestion(metrics, instance.get("DBInstanceClass", ""))

    def is_clickhouse_idle(self, metrics):
        """åˆ¤æ–­ClickHouseå®ä¾‹æ˜¯å¦é—²ç½®"""
        # ClickHouseé—²ç½®åˆ¤æ–­æ ‡å‡†ï¼ˆæˆ–å…³ç³»ï¼‰
        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        disk_util = metrics.get("ç£ç›˜åˆ©ç”¨ç‡", 0)
        connection_count = metrics.get("è¿æ¥æ•°", 0)
        query_count = metrics.get("æŸ¥è¯¢æ•°", 0)
        insert_count = metrics.get("æ’å…¥æ•°", 0)
        network_in = metrics.get("ç½‘ç»œå…¥æµé‡", 0)
        network_out = metrics.get("ç½‘ç»œå‡ºæµé‡", 0)
        disk_read_iops = metrics.get("ç£ç›˜è¯»IOPS", 0)
        disk_write_iops = metrics.get("ç£ç›˜å†™IOPS", 0)

        # é—²ç½®æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³åˆ¤å®šä¸ºé—²ç½®ï¼‰
        idle_conditions = [
            cpu_util < 10,  # CPUåˆ©ç”¨ç‡ä½äº10%
            memory_util < 20,  # å†…å­˜åˆ©ç”¨ç‡ä½äº20%
            disk_util < 20,  # ç£ç›˜åˆ©ç”¨ç‡ä½äº20%
            connection_count < 10,  # è¿æ¥æ•°ä½äº10
            query_count < 100,  # æŸ¥è¯¢æ•°ä½äº100
            insert_count < 50,  # æ’å…¥æ•°ä½äº50
            network_in < 1,  # ç½‘ç»œå…¥æµé‡ä½äº1KB/s
            network_out < 1,  # ç½‘ç»œå‡ºæµé‡ä½äº1KB/s
            disk_read_iops < 100,  # ç£ç›˜è¯»IOPSä½äº100
            disk_write_iops < 100,  # ç£ç›˜å†™IOPSä½äº100
        ]

        return any(idle_conditions)

    def get_idle_reason(self, metrics):
        """è·å–é—²ç½®åŸå› """
        reasons = []

        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        disk_util = metrics.get("ç£ç›˜åˆ©ç”¨ç‡", 0)
        connection_count = metrics.get("è¿æ¥æ•°", 0)
        query_count = metrics.get("æŸ¥è¯¢æ•°", 0)
        insert_count = metrics.get("æ’å…¥æ•°", 0)
        network_in = metrics.get("ç½‘ç»œå…¥æµé‡", 0)
        network_out = metrics.get("ç½‘ç»œå‡ºæµé‡", 0)
        disk_read_iops = metrics.get("ç£ç›˜è¯»IOPS", 0)
        disk_write_iops = metrics.get("ç£ç›˜å†™IOPS", 0)

        if cpu_util < 10:
            reasons.append(f"CPUåˆ©ç”¨ç‡({cpu_util:.1f}%) < 10%")
        if memory_util < 20:
            reasons.append(f"å†…å­˜åˆ©ç”¨ç‡({memory_util:.1f}%) < 20%")
        if disk_util < 20:
            reasons.append(f"ç£ç›˜åˆ©ç”¨ç‡({disk_util:.1f}%) < 20%")
        if connection_count < 10:
            reasons.append(f"è¿æ¥æ•°({connection_count:.0f}) < 10")
        if query_count < 100:
            reasons.append(f"æŸ¥è¯¢æ•°({query_count:.0f}) < 100")
        if insert_count < 50:
            reasons.append(f"æ’å…¥æ•°({insert_count:.0f}) < 50")
        if network_in < 1:
            reasons.append(f"ç½‘ç»œå…¥æµé‡({network_in:.2f}KB/s) < 1KB/s")
        if network_out < 1:
            reasons.append(f"ç½‘ç»œå‡ºæµé‡({network_out:.2f}KB/s) < 1KB/s")
        if disk_read_iops < 100:
            reasons.append(f"ç£ç›˜è¯»IOPS({disk_read_iops:.0f}) < 100")
        if disk_write_iops < 100:
            reasons.append(f"ç£ç›˜å†™IOPS({disk_write_iops:.0f}) < 100")

        return "; ".join(reasons)

    def get_optimization_suggestion(self, metrics, instance_class):
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        cpu_util = metrics.get("CPUåˆ©ç”¨ç‡", 0)
        memory_util = metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0)
        disk_util = metrics.get("ç£ç›˜åˆ©ç”¨ç‡", 0)
        connection_count = metrics.get("è¿æ¥æ•°", 0)
        query_count = metrics.get("æŸ¥è¯¢æ•°", 0)
        insert_count = metrics.get("æ’å…¥æ•°", 0)

        if cpu_util < 10 and memory_util < 20:
            suggestions.append("å»ºè®®é™é…å®ä¾‹è§„æ ¼")
        elif cpu_util < 10:
            suggestions.append("å»ºè®®é™é…CPUè§„æ ¼")
        elif memory_util < 20:
            suggestions.append("å»ºè®®é™é…å†…å­˜è§„æ ¼")

        if disk_util < 20:
            suggestions.append("å»ºè®®å‡å°‘å­˜å‚¨å®¹é‡")

        if connection_count < 10:
            suggestions.append("å»ºè®®ä½¿ç”¨æ›´å°çš„å®ä¾‹ç±»å‹")

        if query_count < 100 and insert_count < 50:
            suggestions.append("å»ºè®®åˆå¹¶åˆ°å…¶ä»–å®ä¾‹æˆ–åˆ é™¤")

        return "; ".join(suggestions) if suggestions else "å»ºè®®ä¿æŒå½“å‰é…ç½®"

    def get_monthly_cost(self, instance_id, instance_class, region):
        """è·å–ClickHouseå®ä¾‹æœˆæˆæœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦è°ƒç”¨BSS APIï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨DescribeRenewalPrice API
        # æ ¹æ®å®ä¾‹ç±»å‹å’ŒåŒºåŸŸè¿”å›ä¼°ç®—æˆæœ¬
        cost_map = {
            "clickhouse.c1.small": 300,
            "clickhouse.c1.medium": 500,
            "clickhouse.c1.large": 800,
            "clickhouse.c1.xlarge": 1200,
            "clickhouse.c2.small": 400,
            "clickhouse.c2.medium": 600,
            "clickhouse.c2.large": 1000,
            "clickhouse.c2.xlarge": 1500,
        }

        return cost_map.get(instance_class, 500)  # é»˜è®¤500å…ƒ

    def analyze(self, regions=None, days=14):
        """åˆ†æèµ„æº (BaseResourceAnalyzeræ¥å£)"""
        # ClickHouseAnalyzerçš„analyze_clickhouse_resourcesé€»è¾‘æ¯”è¾ƒå¤æ‚
        return self.analyze_clickhouse_resources()

    def generate_report(self, idle_instances):
        """ç”ŸæˆæŠ¥å‘Š (BaseResourceAnalyzeræ¥å£)"""
        self.generate_clickhouse_report(idle_instances, self.tenant_name)

    def analyze_clickhouse_resources(self):
        """åˆ†æClickHouseèµ„æº"""
        self.logger.info("å¼€å§‹ClickHouseèµ„æºåˆ†æ...")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()

        # è·å–æ‰€æœ‰åŒºåŸŸ
        regions = self.get_all_regions()
        self.logger.info(f"è·å–åˆ° {len(regions)} ä¸ªåŒºåŸŸ")

        # å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„å®ä¾‹
        self.logger.info("ğŸ” å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„ClickHouseå®ä¾‹...")

        def get_region_instances(region_item):
            """è·å–å•ä¸ªåŒºåŸŸçš„å®ä¾‹ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
            region = region_item
            try:
                instances = self.get_clickhouse_instances(region)
                return {"region": region, "instances": instances}
            except Exception as e:
                return {"region": region, "instances": []}

        # å¹¶å‘è·å–æ‰€æœ‰åŒºåŸŸçš„å®ä¾‹
        region_results = process_concurrently(
            regions, get_region_instances, max_workers=10, description="è·å–ClickHouseå®ä¾‹"
        )

        # æ•´ç†æ‰€æœ‰å®ä¾‹
        all_instances = []
        for result in region_results:
            if result and result.get("instances"):
                all_instances.extend(result["instances"])
                self.logger.info(f"{result['region']}: {len(result['instances'])} ä¸ªå®ä¾‹")

        if not all_instances:
            self.logger.warning("æœªå‘ç°ä»»ä½•ClickHouseå®ä¾‹")
            return []

        self.logger.info(f"æ€»å…±è·å–åˆ° {len(all_instances)} ä¸ªClickHouseå®ä¾‹")

        # å®šä¹‰å•ä¸ªå®ä¾‹å¤„ç†å‡½æ•°ï¼ˆç”¨äºå¹¶å‘ï¼‰
        def process_single_instance(instance_item):
            """å¤„ç†å•ä¸ªå®ä¾‹ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
            instance = instance_item
            instance_id = instance["InstanceId"]
            region = instance["Region"]

            try:
                metrics = self.get_clickhouse_metrics(region, instance_id)
                return {"success": True, "instance_id": instance_id, "metrics": metrics}
            except Exception as e:
                error = ErrorHandler.handle_api_error(e, "ClickHouse", region, instance_id)
                ErrorHandler.handle_instance_error(
                    e, instance_id, region, "ClickHouse", continue_on_error=True
                )
                return {
                    "success": False,
                    "instance_id": instance_id,
                    "metrics": {},
                    "error": str(e),
                }

        # å¹¶å‘è·å–ç›‘æ§æ•°æ®
        self.logger.info("å¹¶å‘è·å–ç›‘æ§æ•°æ®ï¼ˆæœ€å¤š10ä¸ªå¹¶å‘çº¿ç¨‹ï¼‰...")

        def progress_callback(completed, total):
            progress_pct = completed / total * 100
            sys.stdout.write(f"\rğŸ“Š ç›‘æ§æ•°æ®è¿›åº¦: {completed}/{total} ({progress_pct:.1f}%)")
            sys.stdout.flush()

        monitoring_results = process_concurrently(
            all_instances,
            process_single_instance,
            max_workers=10,
            description="ClickHouseç›‘æ§æ•°æ®é‡‡é›†",
            progress_callback=progress_callback,
        )

        # æ¢è¡Œ

        # æ•´ç†ç›‘æ§æ•°æ®
        all_monitoring_data = {}
        success_count = 0
        fail_count = 0

        for result in monitoring_results:
            if result and result.get("success"):
                all_monitoring_data[result["instance_id"]] = result["metrics"]
                success_count += 1
            else:
                if result:
                    instance_id = result.get("instance_id", "unknown")
                    all_monitoring_data[instance_id] = {}
                    fail_count += 1

        self.logger.info(f"ç›‘æ§æ•°æ®è·å–å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")

        # ä¿å­˜æ•°æ®
        self.save_clickhouse_data(all_instances, all_monitoring_data)

        # åˆ†æé—²ç½®å®ä¾‹
        idle_instances = []
        for instance in all_instances:
            instance_id = instance["InstanceId"]
            metrics = all_monitoring_data.get(instance_id, {})

            if self.is_clickhouse_idle(metrics):
                idle_reason = self.get_idle_reason(metrics)
                optimization = self.get_optimization_suggestion(
                    metrics, instance.get("DBInstanceClass", "")
                )
                monthly_cost = self.get_monthly_cost(
                    instance_id, instance.get("DBInstanceClass", ""), instance["Region"]
                )

                idle_instances.append(
                    {
                        "å®ä¾‹ID": instance_id,
                        "å®ä¾‹åç§°": instance["DBInstanceDescription"],
                        "å®ä¾‹ç±»å‹": instance.get("DBInstanceClass", ""),
                        "ç‰ˆæœ¬": instance.get("EngineVersion", ""),
                        "åŒºåŸŸ": instance["Region"],
                        "çŠ¶æ€": instance["DBInstanceStatus"],
                        "CPUåˆ©ç”¨ç‡(%)": metrics.get("CPUåˆ©ç”¨ç‡", 0),
                        "å†…å­˜åˆ©ç”¨ç‡(%)": metrics.get("å†…å­˜åˆ©ç”¨ç‡", 0),
                        "ç£ç›˜åˆ©ç”¨ç‡(%)": metrics.get("ç£ç›˜åˆ©ç”¨ç‡", 0),
                        "è¿æ¥æ•°": metrics.get("è¿æ¥æ•°", 0),
                        "æŸ¥è¯¢æ•°": metrics.get("æŸ¥è¯¢æ•°", 0),
                        "æ’å…¥æ•°": metrics.get("æ’å…¥æ•°", 0),
                        "ç½‘ç»œå…¥æµé‡(KB/s)": metrics.get("ç½‘ç»œå…¥æµé‡", 0),
                        "ç½‘ç»œå‡ºæµé‡(KB/s)": metrics.get("ç½‘ç»œå‡ºæµé‡", 0),
                        "ç£ç›˜è¯»IOPS": metrics.get("ç£ç›˜è¯»IOPS", 0),
                        "ç£ç›˜å†™IOPS": metrics.get("ç£ç›˜å†™IOPS", 0),
                        "é—²ç½®åŸå› ": idle_reason,
                        "ä¼˜åŒ–å»ºè®®": optimization,
                        "æœˆæˆæœ¬(Â¥)": monthly_cost,
                    }
                )

        self.logger.info(f"ClickHouseåˆ†æå®Œæˆ: å‘ç° {len(idle_instances)} ä¸ªé—²ç½®å®ä¾‹")
        return idle_instances

    def generate_clickhouse_report(self, idle_instances, tenant_name=None, output_base_dir="."):
        """ç”ŸæˆClickHouseæŠ¥å‘Š"""
        if not idle_instances:
            self.logger.warning("æ²¡æœ‰å‘ç°é—²ç½®çš„ClickHouseå®ä¾‹")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        tenant_prefix = f"{tenant_name}_" if tenant_name else ""

        # ç”ŸæˆExcelæŠ¥å‘Š
        df = pd.DataFrame(idle_instances)
        excel_file = f"{output_base_dir}/{tenant_prefix}clickhouse_idle_report_{timestamp}.xlsx"
        df.to_excel(excel_file, index=False)
        self.logger.info(f"ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {excel_file}")

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = f"{output_base_dir}/{tenant_prefix}clickhouse_idle_report_{timestamp}.html"
        self.generate_html_report(idle_instances, html_file, tenant_name)
        self.logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        # ç»Ÿè®¡ä¿¡æ¯
        total_cost = sum(instance["æœˆæˆæœ¬(Â¥)"] for instance in idle_instances)
        self.logger.info("ClickHouseé—²ç½®å®ä¾‹ç»Ÿè®¡:")
        self.logger.info(f"  æ€»æ•°é‡: {len(idle_instances)} ä¸ª")
        self.logger.info(f"  æ€»æœˆæˆæœ¬: {total_cost:,.2f} å…ƒ")
        self.logger.info(f"  é¢„è®¡å¹´èŠ‚çœ: {total_cost * 12:,.2f} å…ƒ")

    def generate_html_report(self, idle_instances, filename, tenant_name=None):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        tenant_str = f" - {tenant_name}" if tenant_name else ""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ClickHouseé—²ç½®å®ä¾‹åˆ†ææŠ¥å‘Š{tenant_str}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1600px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
        .summary {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}
        table {{ width: 100%; border-collapse: collapse; margin-top: 20px; font-size: 12px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #3498db; color: white; position: sticky; top: 0; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .metric {{ font-weight: bold; color: #e74c3c; }}
        .low-utilization {{ background-color: #fff3cd; }}
        .footer {{ margin-top: 30px; padding: 15px; background: #34495e; color: white; text-align: center; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š ClickHouseé—²ç½®å®ä¾‹åˆ†ææŠ¥å‘Š{tenant_str}</h1>
        
        <div class="summary">
            <h3>ğŸ“Š æŠ¥å‘Šæ‘˜è¦</h3>
            <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>é—²ç½®å®ä¾‹æ•°é‡:</strong> {len(idle_instances)} ä¸ª</p>
            <p><strong>æ€»æœˆæˆæœ¬:</strong> {sum(instance['æœˆæˆæœ¬(Â¥)'] for instance in idle_instances):,.2f} å…ƒ</p>
            <p><strong>é¢„è®¡å¹´èŠ‚çœ:</strong> {sum(instance['æœˆæˆæœ¬(Â¥)'] for instance in idle_instances) * 12:,.2f} å…ƒ</p>
        </div>
        
        <table>
            <thead>
                <tr>
                    <th>å®ä¾‹åç§°</th>
                    <th>å®ä¾‹ID</th>
                    <th>å®ä¾‹ç±»å‹</th>
                    <th>ç‰ˆæœ¬</th>
                    <th>åŒºåŸŸ</th>
                    <th>çŠ¶æ€</th>
                    <th>CPUåˆ©ç”¨ç‡(%)</th>
                    <th>å†…å­˜åˆ©ç”¨ç‡(%)</th>
                    <th>ç£ç›˜åˆ©ç”¨ç‡(%)</th>
                    <th>è¿æ¥æ•°</th>
                    <th>æŸ¥è¯¢æ•°</th>
                    <th>æ’å…¥æ•°</th>
                    <th>ç½‘ç»œå…¥æµé‡(KB/s)</th>
                    <th>ç½‘ç»œå‡ºæµé‡(KB/s)</th>
                    <th>ç£ç›˜è¯»IOPS</th>
                    <th>ç£ç›˜å†™IOPS</th>
                    <th>é—²ç½®åŸå› </th>
                    <th>ä¼˜åŒ–å»ºè®®</th>
                    <th>æœˆæˆæœ¬(Â¥)</th>
                </tr>
            </thead>
            <tbody>
"""

        for instance in idle_instances:
            html_content += f"""
                <tr>
                    <td>{instance['å®ä¾‹åç§°']}</td>
                    <td>{instance['å®ä¾‹ID']}</td>
                    <td>{instance['å®ä¾‹ç±»å‹']}</td>
                    <td>{instance['ç‰ˆæœ¬']}</td>
                    <td>{instance['åŒºåŸŸ']}</td>
                    <td>{instance['çŠ¶æ€']}</td>
                    <td><span class="metric">{instance['CPUåˆ©ç”¨ç‡(%)']:.1f}%</span></td>
                    <td><span class="metric">{instance['å†…å­˜åˆ©ç”¨ç‡(%)']:.1f}%</span></td>
                    <td><span class="metric">{instance['ç£ç›˜åˆ©ç”¨ç‡(%)']:.1f}%</span></td>
                    <td>{instance['è¿æ¥æ•°']:.0f}</td>
                    <td>{instance['æŸ¥è¯¢æ•°']:.0f}</td>
                    <td>{instance['æ’å…¥æ•°']:.0f}</td>
                    <td>{instance['ç½‘ç»œå…¥æµé‡(KB/s)']:.2f}</td>
                    <td>{instance['ç½‘ç»œå‡ºæµé‡(KB/s)']:.2f}</td>
                    <td>{instance['ç£ç›˜è¯»IOPS']:.0f}</td>
                    <td>{instance['ç£ç›˜å†™IOPS']:.0f}</td>
                    <td>{instance['é—²ç½®åŸå› ']}</td>
                    <td>{instance['ä¼˜åŒ–å»ºè®®']}</td>
                    <td>{instance['æœˆæˆæœ¬(Â¥)']:.2f}</td>
                </tr>
"""

        html_content += """
            </tbody>
        </table>
        
        <div class="footer">
            <p>æŠ¥å‘Šç”±é˜¿é‡Œäº‘èµ„æºåˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ</p>
        </div>
    </div>
</body>
</html>
"""

        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_content)


def main():
    """ä¸»å‡½æ•°"""
    import sys

    if len(sys.argv) < 3:
        self.logger.info("ç”¨æ³•: python clickhouse_analyzer.py <access_key_id> <access_key_secret>")
        sys.exit(1)

    access_key_id = sys.argv[1]
    access_key_secret = sys.argv[2]

    analyzer = ClickHouseAnalyzer(access_key_id, access_key_secret)
    idle_instances = analyzer.analyze_clickhouse_resources()
    analyzer.generate_clickhouse_report(idle_instances)


if __name__ == "__main__":
    main()
